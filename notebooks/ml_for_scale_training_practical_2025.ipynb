{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYR3RrYZI30J"
      },
      "source": [
        "# PRATICAL 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tID1CmE-GON7"
      },
      "source": [
        "# import the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a6N-L7V2H1J7",
        "outputId": "a397a73c-6cec-4420-8a71-17c5f8c861cc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc  # Garbage Collector\n",
        "\n",
        "# --- 1. Setup Paths & Configuration ---\n",
        "BASE_DIR = '/content/drive/MyDrive/ML_at_scale'\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'ml-32m')\n",
        "FIGURES_DIR = os.path.join(BASE_DIR, 'figures')\n",
        "\n",
        "# Create the figures directory if it doesn't exist\n",
        "if not os.path.exists(FIGURES_DIR):\n",
        "    os.makedirs(FIGURES_DIR)\n",
        "    print(f\"[-] Created directory: {FIGURES_DIR}\")\n",
        "else:\n",
        "    print(f\"[-] Saving results to: {FIGURES_DIR}\")\n",
        "\n",
        "# Plotting Style\n",
        "sns.set_theme(style=\"whitegrid\", context=\"paper\", font_scale=1.4)\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "# We save as PDF for reports (vector quality) and display PNG inline\n",
        "\n",
        "# --- 2. Optimized Data Loading ---\n",
        "print(\"[-] Loading data (this may take 1-2 minutes)...\")\n",
        "\n",
        "ratings_path = os.path.join(DATA_DIR, 'ratings.csv')\n",
        "movies_path = os.path.join(DATA_DIR, 'movies.csv')\n",
        "\n",
        "# Optimization: Int32 and Float32 reduce RAM usage by 50%\n",
        "dtypes_ratings = {\n",
        "    'userId': 'int32',\n",
        "    'movieId': 'int32',\n",
        "    'rating': 'float32',\n",
        "    'timestamp': 'int64'\n",
        "}\n",
        "\n",
        "df_ratings = pd.read_csv(ratings_path, dtype=dtypes_ratings)\n",
        "df_movies = pd.read_csv(movies_path, dtype={'movieId': 'int32'})\n",
        "\n",
        "# Convert timestamp to date object\n",
        "print(\"[-] Converting timestamps...\")\n",
        "df_ratings['date'] = pd.to_datetime(df_ratings['timestamp'], unit='s')\n",
        "\n",
        "print(f\"[+] Data Loaded. {len(df_ratings):,} ratings.\")\n",
        "\n",
        "# =========================================================\n",
        "#  PLOT 1: Global Rating Distribution\n",
        "# =========================================================\n",
        "print(\"[-] Generating Plot 1: Rating Distribution...\")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "counts = df_ratings['rating'].value_counts().sort_index()\n",
        "sns.barplot(x=counts.index, y=counts.values, palette=\"viridis\")\n",
        "\n",
        "plt.title(\"Global Rating Distribution\")\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save\n",
        "save_path = os.path.join(FIGURES_DIR, 'p0_rating_distribution.pdf')\n",
        "plt.savefig(save_path, format='pdf', bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"    Saved to: {save_path}\")\n",
        "\n",
        "# =========================================================\n",
        "# PLOT 2: Long Tail (Power Law)\n",
        "# =========================================================\n",
        "print(\"[-] Generating Plot 2: Long Tail Distribution...\")\n",
        "\n",
        "# Get rating counts per movie\n",
        "item_counts = df_ratings['movieId'].value_counts().values # Sorted Descending\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(item_counts, color='#2E86C1', linewidth=2, label=\"Actual Data\")\n",
        "\n",
        "# Theoretical Reference Line (Power Law: y = k * x^-alpha)\n",
        "x = np.arange(1, len(item_counts) + 1)\n",
        "alpha = 0.8  # Typical slope for social data\n",
        "y_ref = item_counts[0] * (x ** -alpha)\n",
        "plt.plot(x, y_ref, 'r--', label=r'Power Law Ref ($\\alpha \\approx 0.8$)')\n",
        "\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "plt.title(\"Long Tail Distribution (Item Popularity)\")\n",
        "plt.xlabel(\"Item Rank (Log)\")\n",
        "plt.ylabel(\"Number of Ratings (Log)\")\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save\n",
        "save_path = os.path.join(FIGURES_DIR, 'p0_long_tail.pdf')\n",
        "plt.savefig(save_path, format='pdf', bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"    Saved to: {save_path}\")\n",
        "\n",
        "# =========================================================\n",
        "#  PLOT 3: Temporal Evolution\n",
        "# =========================================================\n",
        "print(\"[-] Generating Plot 3: Temporal Evolution...\")\n",
        "\n",
        "# Resample by Month End ('ME')\n",
        "monthly_counts = df_ratings.set_index('date').resample('ME').size()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(monthly_counts.index, monthly_counts.values, color='purple', linewidth=2)\n",
        "\n",
        "plt.title(\"Evolution of Ratings Over Time\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Ratings per Month\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save\n",
        "save_path = os.path.join(FIGURES_DIR, 'p0_temporal_evolution.pdf')\n",
        "plt.savefig(save_path, format='pdf', bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"    Saved to: {save_path}\")\n",
        "\n",
        "# =========================================================\n",
        "#  PLOT 4: Sparsity Heatmap (Top 50x50)\n",
        "# =========================================================\n",
        "print(\"[-] Generating Plot 4: Interaction Heatmap...\")\n",
        "\n",
        "top_n = 50\n",
        "top_users = df_ratings['userId'].value_counts().index[:top_n]\n",
        "top_items = df_ratings['movieId'].value_counts().index[:top_n]\n",
        "\n",
        "# Filter Data (Crucial step for RAM)\n",
        "mask = (df_ratings['userId'].isin(top_users)) & (df_ratings['movieId'].isin(top_items))\n",
        "df_small = df_ratings[mask]\n",
        "\n",
        "# Create Pivot Table\n",
        "heatmap_matrix = df_small.pivot(index='userId', columns='movieId', values='rating')\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(heatmap_matrix, cbar=False, cmap=\"YlGnBu\", linewidths=0.05, linecolor='gray')\n",
        "\n",
        "plt.title(f\"Interaction Matrix (Top {top_n} Users x Top {top_n} Items)\")\n",
        "plt.xlabel(\"Popular Movies\")\n",
        "plt.ylabel(\"Active Users\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save\n",
        "save_path = os.path.join(FIGURES_DIR, 'p0_sparsity_heatmap.pdf')\n",
        "plt.savefig(save_path, format='pdf', bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"    Saved to: {save_path}\")\n",
        "\n",
        "# =========================================================\n",
        "#  PLOT 5: Popularity vs Quality Correlation\n",
        "# =========================================================\n",
        "print(\"[-] Generating Plot 5: Correlation...\")\n",
        "\n",
        "# Group by movie\n",
        "movie_stats = df_ratings.groupby('movieId').agg({'rating': ['count', 'mean']})\n",
        "movie_stats.columns = ['count', 'mean']\n",
        "\n",
        "# Filter noise (movies with < 50 ratings)\n",
        "robust_movies = movie_stats[movie_stats['count'] > 50]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='count', y='mean', data=robust_movies, alpha=0.3, edgecolor=None, color=\"#C0392B\")\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.title(\"Correlation: Popularity vs Average Rating\")\n",
        "plt.xlabel(\"Number of Ratings (Log)\")\n",
        "plt.ylabel(\"Average Rating\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save\n",
        "save_path = os.path.join(FIGURES_DIR, 'p0_correlation.pdf')\n",
        "plt.savefig(save_path, format='pdf', bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"    Saved to: {save_path}\")\n",
        "\n",
        "print(\" Practical 0 Complete. All files saved to Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-jIRIc4E40f"
      },
      "outputs": [],
      "source": [
        "movies_df=pd.read_csv('/content/drive/MyDrive/ML_at_scale/ml-32m/movies.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "welXWTATEpwF"
      },
      "outputs": [],
      "source": [
        "links_df=pd.read_csv('/content/drive/MyDrive/ML_at_scale/ml-32m/links.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW2hdM_-EX6J"
      },
      "outputs": [],
      "source": [
        "ratings_df=pd.read_csv('/content/drive/MyDrive/ML_at_scale/ml-32m/ratings.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNqCU33kDBIK"
      },
      "outputs": [],
      "source": [
        "tags_df=pd.read_csv('/content/drive/MyDrive/ML_at_scale/ml-32m/tags.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HeWmKz-CRwqz",
        "outputId": "ad8b314e-0299-4652-8b6f-08f18dbbe235"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_DIR = '/content/drive/MyDrive/ML_at_scale'\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'ml-32m')\n",
        "FIGURES_DIR = os.path.join(BASE_DIR, 'figures')\n",
        "RATINGS_PATH = os.path.join(DATA_DIR, 'ratings.csv')\n",
        "\n",
        "# Style\n",
        "sns.set_theme(style=\"whitegrid\", context=\"paper\", font_scale=1.4)\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "print(\"[-] Loading dataset (Pandas)...\")\n",
        "df = pd.read_csv(RATINGS_PATH, dtype={\n",
        "    'userId': 'int32',\n",
        "    'movieId': 'int32',\n",
        "    'rating': 'float32'\n",
        "})\n",
        "print(f\"[+] Loaded {len(df):,} ratings.\")\n",
        "\n",
        "# --- 2. ID Mapping (The \"Map\" circle on the board) ---\n",
        "print(\"[-] Building ID Mappings...\")\n",
        "\n",
        "# Unique sorted IDs\n",
        "unique_users = np.unique(df['userId'])\n",
        "unique_items = np.unique(df['movieId'])\n",
        "\n",
        "n_users = len(unique_users)\n",
        "n_items = len(unique_items)\n",
        "\n",
        "#  Board Logic: userid_to_idx = {\"A\": 0, \"B\": 1}\n",
        "userid_to_idx = {original: i for i, original in enumerate(unique_users)}\n",
        "movieid_to_idx = {original: i for i, original in enumerate(unique_items)}\n",
        "\n",
        "# Board Logic: idx_to_userid = [\"A\", \"B\"]\n",
        "idx_to_userid = list(unique_users)\n",
        "idx_to_movieid = list(unique_items)\n",
        "\n",
        "# Apply mapping to DataFrame for faster iteration\n",
        "print(\"[-] Converting DataFrame columns to internal indices...\")\n",
        "df['u_idx'] = df['userId'].map(userid_to_idx)\n",
        "df['i_idx'] = df['movieId'].map(movieid_to_idx)\n",
        "\n",
        "# --- 3. Building Adjacency Lists (The main part of the board) ---\n",
        "print(f\"[-] Building 'List of Lists' structures for {n_users} users and {n_items} items...\")\n",
        "t0 = time.time()\n",
        "\n",
        "# Initialize empty lists of lists\n",
        "# data_by_user = [ [], [], ... ]\n",
        "data_by_user = [[] for _ in range(n_users)]\n",
        "\n",
        "# data_by_movie = [ [], [], ... ]\n",
        "data_by_movie = [[] for _ in range(n_items)]\n",
        "\n",
        "# Iterate efficiently using zip (much faster than iterrows)\n",
        "# We fill both lists in one pass\n",
        "# Row structure: (user_idx, item_idx, rating)\n",
        "for u, i, r in zip(df['u_idx'], df['i_idx'], df['rating']):\n",
        "    # Logic: data_by_user[0] = [(0, 5), (1, 2)]\n",
        "    data_by_user[u].append((i, r))\n",
        "\n",
        "    # Logic: data_by_movie[0] = [(0, 5)]\n",
        "    data_by_movie[i].append((u, r))\n",
        "\n",
        "print(f\"[+] Structures built in {time.time() - t0:.2f} seconds.\")\n",
        "\n",
        "# --- Verification (Print first few like the board) ---\n",
        "print(\"\\n--- Board Verification (First User) ---\")\n",
        "u_idx_0 = 0\n",
        "print(f\"idx_to_userid[{u_idx_0}] = {idx_to_userid[u_idx_0]}\")\n",
        "print(f\"data_by_user[{u_idx_0}] (First 5): {data_by_user[u_idx_0][:5]}\")\n",
        "\n",
        "print(\"\\n--- Board Verification (First Movie) ---\")\n",
        "i_idx_0 = 0\n",
        "print(f\"idx_to_movieid[{i_idx_0}] = {idx_to_movieid[i_idx_0]}\")\n",
        "print(f\"data_by_movie[{i_idx_0}] (First 5): {data_by_movie[i_idx_0][:5]}\")\n",
        "\n",
        "\n",
        "# --- 4. Power Law Analysis using Lists ---\n",
        "print(\"\\n[-] Calculating Degrees for Plots...\")\n",
        "\n",
        "# To get degree, we just take the length of the list!\n",
        "# len(data_by_user[u]) is the number of ratings user u gave.\n",
        "user_degrees = [len(ratings) for ratings in data_by_user]\n",
        "item_degrees = [len(ratings) for ratings in data_by_movie]\n",
        "\n",
        "# --- PLOT 1: Item Power Law ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "sorted_item_degrees = np.sort(item_degrees)[::-1]\n",
        "plt.plot(sorted_item_degrees, color='#2980B9', linewidth=2, label=\"Movie Popularity\")\n",
        "\n",
        "# Ref line\n",
        "x = np.arange(1, len(sorted_item_degrees) + 1)\n",
        "y_ref = sorted_item_degrees[0] * (x ** -0.8)\n",
        "plt.plot(x, y_ref, 'r--', label=r'Power Law Ref ($\\alpha \\approx 0.8$)')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.title(\"Item Degree Distribution (Using List of Lists)\")\n",
        "plt.xlabel(\"Item Rank (Log)\")\n",
        "plt.ylabel(\"Degree (Log)\")\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
        "plt.savefig(os.path.join(FIGURES_DIR, 'p1_list_power_law_items.pdf'), bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# --- PLOT 2: User Power Law ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "sorted_user_degrees = np.sort(user_degrees)[::-1]\n",
        "plt.plot(sorted_user_degrees, color='#27AE60', linewidth=2, label=\"User Activity\")\n",
        "plt.axhline(y=20, color='orange', linestyle=':', label=\"Truncation (20)\")\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.title(\"User Degree Distribution (Using List of Lists)\")\n",
        "plt.xlabel(\"User Rank (Log)\")\n",
        "plt.ylabel(\"Degree (Log)\")\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
        "plt.savefig(os.path.join(FIGURES_DIR, 'p1_list_power_law_users.pdf'), bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "id": "cT_fC77eRzXl",
        "outputId": "5f6f84a8-6cc7-4711-9ed2-6aef79cdb41e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import gc\n",
        "\n",
        "# --- 1. Load Data into Sparse Matrix ---\n",
        "print(\"[-] Loading dataset into Sparse Matrix...\")\n",
        "\n",
        "class CorrectedBiasALS:\n",
        "    def __init__(self, lambda_reg=3.03811, n_epochs=10):\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.n_epochs = n_epochs\n",
        "        self.mu = 0.0\n",
        "        self.b_u = None\n",
        "        self.b_i = None\n",
        "        self.loss_history = []\n",
        "        self.train_rmse_history = []\n",
        "        self.test_rmse_history = []\n",
        "\n",
        "    def fit(self, R_train, R_test):\n",
        "        n_u, n_i = R_train.shape\n",
        "\n",
        "        # 1. Initialisation\n",
        "        self.mu = np.mean(R_train.data)\n",
        "        self.b_u = np.zeros(n_u, dtype=np.float32)\n",
        "        self.b_i = np.zeros(n_i, dtype=np.float32)\n",
        "\n",
        "        # 2. Cr√©ation de la Matrice Binaire (P)\n",
        "        # P[u,i] = 1 si not√©, 0 sinon.\n",
        "        # Indispensable pour sommer les biais correctement sans multiplier par la note.\n",
        "        print(\"    Cr√©ation de la matrice binaire (Indicateur)...\")\n",
        "        P_train = R_train.copy()\n",
        "        P_train.data = np.ones_like(P_train.data, dtype=np.float32)\n",
        "\n",
        "        # Comptes (D√©nominateurs)\n",
        "        user_counts = np.array(R_train.getnnz(axis=1)).flatten().astype(np.float32)\n",
        "\n",
        "        print(\"    Conversion CSC...\")\n",
        "        R_csc = R_train.tocsc()\n",
        "        P_csc = P_train.tocsc() # Version binaire en colonnes\n",
        "        item_counts = np.array(R_train.getnnz(axis=0)).flatten().astype(np.float32)\n",
        "\n",
        "        print(f\"    D√©but de l'entra√Ænement (Lambda={self.lambda_reg})...\")\n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "            t0 = time.time()\n",
        "\n",
        "            # --- UPDATE USER BIAS ---\n",
        "            # b_u = (sum(r_ui - mu - b_i)) / (count + lambda)\n",
        "\n",
        "            # A. Somme des (r_ui - mu)\n",
        "            # Astuce : sum(r_ui) - count * mu\n",
        "            sum_ratings = np.array(R_train.sum(axis=1)).flatten()\n",
        "\n",
        "            # B. Somme des b_i (CORRECTION ICI)\n",
        "            # On utilise P_train (binaire) au lieu de R_train (notes)\n",
        "            sum_bi = P_train.dot(self.b_i)\n",
        "\n",
        "            numer = sum_ratings - (user_counts * self.mu) - sum_bi\n",
        "            denom = user_counts + self.lambda_reg\n",
        "            self.b_u = np.divide(numer, denom, where=denom!=0)\n",
        "\n",
        "            # --- UPDATE ITEM BIAS ---\n",
        "            # b_i = (sum(r_ui - mu - b_u)) / (count + lambda)\n",
        "\n",
        "            sum_ratings_i = np.array(R_csc.sum(axis=0)).flatten()\n",
        "\n",
        "            # CORRECTION ICI : On utilise P_train transpos√©e\n",
        "            sum_bu = P_train.T.dot(self.b_u)\n",
        "\n",
        "            numer = sum_ratings_i - (item_counts * self.mu) - sum_bu\n",
        "            denom = item_counts + self.lambda_reg\n",
        "            self.b_i = np.divide(numer, denom, where=denom!=0)\n",
        "\n",
        "            # --- METRICS ---\n",
        "            # Calculer la Loss √† chaque it√©ration est lourd sur 32M\n",
        "            # On calcule seulement le RMSE\n",
        "            train_rmse = self._compute_rmse(R_train)\n",
        "            test_rmse = self._compute_rmse(R_test)\n",
        "\n",
        "            self.train_rmse_history.append(train_rmse)\n",
        "            self.test_rmse_history.append(test_rmse)\n",
        "\n",
        "            print(f\"    Epoch {epoch+1}: Train RMSE={train_rmse:.4f} | Test RMSE={test_rmse:.4f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "    def _compute_rmse(self, R):\n",
        "        if R.nnz == 0: return 0.0\n",
        "        rows, cols = R.nonzero()\n",
        "        # Pr√©diction vectoris√©e\n",
        "        pred = self.mu + self.b_u[rows] + self.b_i[cols]\n",
        "        error = R.data - pred\n",
        "        return np.sqrt(np.mean(error**2))\n",
        "\n",
        "# ==========================================\n",
        "# EX√âCUTION\n",
        "# ==========================================\n",
        "\n",
        "# On utilise Lambda=0.5 qui est standard pour MovieLens\n",
        "model = CorrectedBiasALS(lambda_reg=0.5, n_epochs=10)\n",
        "model.fit(R_train, R_test)\n",
        "\n",
        "# ==========================================\n",
        "# PLOTTING\n",
        "# ==========================================\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(model.train_rmse_history, 'b-o', label=\"Train RMSE\")\n",
        "plt.plot(model.test_rmse_history, 'r-s', label=\"Test RMSE\")\n",
        "plt.title(\"Convergence Bias-Only ALS (Corrig√©)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('/content/drive/MyDrive/ML_at_scale/figures/p2_corrected_rmse.pdf')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final Train RMSE: {model.train_rmse_history[-1]:.4f}\")\n",
        "print(f\"Final Test RMSE: {model.test_rmse_history[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tmFIwT5FZBn0",
        "outputId": "f85120b8-def9-4758-d917-368e28ae0b28"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def tune_lambda(R_train, R_test, candidate_lambdas):\n",
        "    \"\"\"\n",
        "    Performs a Grid Search to find the best regularization parameter (lambda).\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    best_rmse = float('inf')\n",
        "    best_lambda = None\n",
        "\n",
        "    print(f\"[-] Starting Grid Search on: {candidate_lambdas}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Lambda':<10} | {'Train RMSE':<12} | {'Test RMSE':<12} | {'Time (s)':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for l_reg in candidate_lambdas:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Initialize and Train\n",
        "        # We only need ~5 epochs to see convergence for tuning\n",
        "        model = CorrectedBiasALS(lambda_reg=l_reg, n_epochs=5)\n",
        "        model.fit(R_train, R_test)\n",
        "\n",
        "        # Get final metrics\n",
        "        final_train_rmse = model.train_rmse_history[-1]\n",
        "        final_test_rmse = model.test_rmse_history[-1]\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        # Store results\n",
        "        results.append((l_reg, final_train_rmse, final_test_rmse))\n",
        "\n",
        "        print(f\"{l_reg:<10.2f} | {final_train_rmse:<12.4f} | {final_test_rmse:<12.4f} | {duration:<10.1f}\")\n",
        "\n",
        "        # Track best\n",
        "        if final_test_rmse < best_rmse:\n",
        "            best_rmse = final_test_rmse\n",
        "            best_lambda = l_reg\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"üèÜ BEST LAMBDA: {best_lambda} (RMSE: {best_rmse:.4f})\")\n",
        "    return best_lambda, results\n",
        "\n",
        "# ==========================================\n",
        "# 1. EXECUTE TUNING\n",
        "# ==========================================\n",
        "\n",
        "# Candidates to test.\n",
        "# Theory:\n",
        "# - Too low (<0.01) -> Overfitting (Model memorizes noise)\n",
        "# - Too high (>10)  -> Underfitting (Biases shrunk towards 0)\n",
        "candidates = [0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
        "\n",
        "best_lambda, experiment_results = tune_lambda(R_train, R_test, candidates)\n",
        "\n",
        "# ==========================================\n",
        "# 2. VISUALIZE TUNING (Important for Report)\n",
        "# ==========================================\n",
        "lambdas = [x[0] for x in experiment_results]\n",
        "test_errors = [x[2] for x in experiment_results]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(lambdas, test_errors, 'o-', color='purple', linewidth=2, label='Test RMSE')\n",
        "\n",
        "# Highlight Best\n",
        "plt.axvline(x=best_lambda, color='green', linestyle='--', label=f'Optimal $\\lambda={best_lambda}$')\n",
        "\n",
        "plt.xscale('log') # Log scale is better for lambda\n",
        "plt.title(\"Hyperparameter Tuning: Effect of Regularization ($\\lambda$)\")\n",
        "plt.xlabel(\"Regularization Parameter $\\lambda$ (Log Scale)\")\n",
        "plt.ylabel(\"Test RMSE\")\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
        "\n",
        "save_path = '/content/drive/MyDrive/ML_at_scale/figures/p2_hyperparameter_tuning.pdf'\n",
        "plt.savefig(save_path, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 3. TRAIN FINAL MODEL\n",
        "# ==========================================\n",
        "print(f\"[-] Training Final Model with Best Lambda = {best_lambda}...\")\n",
        "final_model = CorrectedBiasALS(lambda_reg=best_lambda, n_epochs=15)\n",
        "final_model.fit(R_train, R_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VlHdLUeBaC0f",
        "outputId": "05850ca6-0ac5-4cf9-bf99-2921926c7ed8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# --- Paths Configuration ---\n",
        "BASE_DIR = '/content/drive/MyDrive/ML_at_scale'\n",
        "FIGURES_DIR = os.path.join(BASE_DIR, 'figures')\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'ml-32m')\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# ==========================================\n",
        "# 1. TUNING FUNCTION\n",
        "# ==========================================\n",
        "def tune_lambda(R_train, R_test, candidate_lambdas):\n",
        "    \"\"\"\n",
        "    Performs a Grid Search to find the best regularization parameter (lambda).\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    best_rmse = float('inf')\n",
        "    best_lambda = None\n",
        "\n",
        "    print(f\"[-] Starting Grid Search on: {candidate_lambdas}\")\n",
        "    print(\"-\" * 65)\n",
        "    print(f\"{'Lambda':<10} | {'Train RMSE':<12} | {'Test RMSE':<12} | {'Time (s)':<10}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    for l_reg in candidate_lambdas:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Train for fewer epochs just to check convergence direction\n",
        "        model = CorrectedBiasALS(lambda_reg=l_reg, n_epochs=5)\n",
        "        model.fit(R_train, R_test)\n",
        "\n",
        "        final_train_rmse = model.train_rmse_history[-1]\n",
        "        final_test_rmse = model.test_rmse_history[-1]\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        results.append((l_reg, final_train_rmse, final_test_rmse))\n",
        "        print(f\"{l_reg:<10.2f} | {final_train_rmse:<12.4f} | {final_test_rmse:<12.4f} | {duration:<10.1f}\")\n",
        "\n",
        "        if final_test_rmse < best_rmse:\n",
        "            best_rmse = final_test_rmse\n",
        "            best_lambda = l_reg\n",
        "\n",
        "    print(\"-\" * 65)\n",
        "    print(f\"üèÜ BEST LAMBDA: {best_lambda} (RMSE: {best_rmse:.4f})\")\n",
        "    return best_lambda, results\n",
        "\n",
        "# ==========================================\n",
        "# 2. EXECUTE GRID SEARCH\n",
        "# ==========================================\n",
        "candidates = [0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
        "\n",
        "# Run Tuning\n",
        "best_lambda, experiment_results = tune_lambda(R_train, R_test, candidates)\n",
        "\n",
        "# ==========================================\n",
        "# 3. SAVE TUNING PLOT\n",
        "# ==========================================\n",
        "lambdas = [x[0] for x in experiment_results]\n",
        "test_errors = [x[2] for x in experiment_results]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(lambdas, test_errors, 'o-', color='purple', linewidth=2, label='Test RMSE')\n",
        "plt.axvline(x=best_lambda, color='green', linestyle='--', label=f'Optimal $\\lambda={best_lambda}$')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.title(\"Hyperparameter Tuning: Effect of Regularization ($\\lambda$)\")\n",
        "plt.xlabel(\"Regularization Parameter $\\lambda$ (Log Scale)\")\n",
        "plt.ylabel(\"Test RMSE\")\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
        "\n",
        "# Save to Figures Directory\n",
        "tune_plot_path = os.path.join(FIGURES_DIR, 'p2_hyperparameter_tuning.pdf')\n",
        "plt.savefig(tune_plot_path, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"[+] Tuning plot saved to: {tune_plot_path}\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. TRAIN & SAVE FINAL MODEL\n",
        "# ==========================================\n",
        "print(f\"\\n[-] Training Final Model with Best Lambda = {best_lambda}...\")\n",
        "\n",
        "final_model = CorrectedBiasALS(lambda_reg=best_lambda, n_epochs=15)\n",
        "final_model.fit(R_train, R_test)\n",
        "\n",
        "# Plot Final Convergence\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(final_model.train_rmse_history, 'b-o', label=\"Train RMSE\")\n",
        "plt.plot(final_model.test_rmse_history, 'r-s', label=\"Test RMSE\")\n",
        "plt.title(f\"Final Convergence (Bias-Only, $\\lambda={best_lambda}$)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Save Final Convergence Plot\n",
        "conv_plot_path = os.path.join(FIGURES_DIR, 'p2_final_convergence.pdf')\n",
        "plt.savefig(conv_plot_path, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"[+] Final convergence plot saved to: {conv_plot_path}\")\n",
        "\n",
        "# Save Model Weights (Pickle)\n",
        "# We need this for the Streamlit App later!\n",
        "model_path = os.path.join(DATA_DIR, 'p2_bias_model.pkl')\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'mu': final_model.mu,\n",
        "        'b_u': final_model.b_u,\n",
        "        'b_i': final_model.b_i,\n",
        "        'lambda': best_lambda\n",
        "    }, f)\n",
        "\n",
        "print(f\"[+] Final model saved to: {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nWUjUa-ybOei",
        "outputId": "486c06cd-4060-45b0-fabe-2dec2b3104db"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# --- Paths Configuration ---\n",
        "BASE_DIR = '/content/drive/MyDrive/ML_at_scale'\n",
        "FIGURES_DIR = os.path.join(BASE_DIR, 'figures')\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'ml-32m')\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# ==========================================\n",
        "# 1. RANDOM SEARCH FUNCTION\n",
        "# ==========================================\n",
        "def random_search_lambda(R_train, R_test, n_iter=10):\n",
        "    \"\"\"\n",
        "    Performs Random Search for Lambda using a Log-Uniform distribution.\n",
        "    Range: 10^-2 (0.01) to 10^1.3 (~20)\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    best_rmse = float('inf')\n",
        "    best_lambda = None\n",
        "\n",
        "    print(f\"[-] Starting Random Search ({n_iter} iterations)...\")\n",
        "    print(\"-\" * 65)\n",
        "    print(f\"{'Iter':<5} | {'Lambda':<10} | {'Train RMSE':<12} | {'Test RMSE':<12} | {'Time (s)':<10}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    np.random.seed(42) # Reproducibility\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # SAMPLE LAMBDA (Log-Uniform)\n",
        "        # We sample exponent x from [-1.5, 1.3] -> Lambda from [0.03, 20]\n",
        "        exponent = np.random.uniform(-1.5, 1.3)\n",
        "        l_reg = 10 ** exponent\n",
        "\n",
        "        # Train for 5 epochs (Enough to see trend)\n",
        "        model = CorrectedBiasALS(lambda_reg=l_reg, n_epochs=5)\n",
        "        model.fit(R_train, R_test)\n",
        "\n",
        "        final_train_rmse = model.train_rmse_history[-1]\n",
        "        final_test_rmse = model.test_rmse_history[-1]\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        results.append((l_reg, final_test_rmse))\n",
        "\n",
        "        print(f\"{i+1:<5} | {l_reg:<10.4f} | {final_train_rmse:<12.4f} | {final_test_rmse:<12.4f} | {duration:<10.1f}\")\n",
        "\n",
        "        if final_test_rmse < best_rmse:\n",
        "            best_rmse = final_test_rmse\n",
        "            best_lambda = l_reg\n",
        "\n",
        "    print(\"-\" * 65)\n",
        "    print(f\"üèÜ BEST LAMBDA: {best_lambda:.5f} (RMSE: {best_rmse:.4f})\")\n",
        "    return best_lambda, results\n",
        "\n",
        "# ==========================================\n",
        "# 2. EXECUTE RANDOM SEARCH\n",
        "# ==========================================\n",
        "# We run 12 iterations to get a good spread\n",
        "best_lambda_rand, experiment_results = random_search_lambda(R_train, R_test, n_iter=12)\n",
        "\n",
        "# ==========================================\n",
        "# 3. VISUALIZE RANDOM SEARCH\n",
        "# ==========================================\n",
        "lambdas = [x[0] for x in experiment_results]\n",
        "test_errors = [x[1] for x in experiment_results]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Use Scatter plot for Random Search (points are not ordered)\n",
        "plt.scatter(lambdas, test_errors, color='purple', s=100, label='Trial Result', alpha=0.7)\n",
        "\n",
        "# Highlight Best\n",
        "plt.scatter([best_lambda_rand], [min(test_errors)], color='red', s=150, edgecolors='black', label='Best Found')\n",
        "plt.axvline(x=best_lambda_rand, color='green', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.title(\"Random Search: Regularization $\\lambda$ vs RMSE\")\n",
        "plt.xlabel(\"Regularization Parameter $\\lambda$ (Log Scale)\")\n",
        "plt.ylabel(\"Test RMSE\")\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
        "\n",
        "# Save Plot\n",
        "rs_plot_path = os.path.join(FIGURES_DIR, 'p2_random_search_tuning.pdf')\n",
        "plt.savefig(rs_plot_path, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"[+] Random Search plot saved to: {rs_plot_path}\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. TRAIN & SAVE FINAL MODEL (From Random Search)\n",
        "# ==========================================\n",
        "print(f\"\\n[-] Training Final Model with Random Search Best Lambda = {best_lambda_rand:.5f}...\")\n",
        "\n",
        "final_model = CorrectedBiasALS(lambda_reg=best_lambda_rand, n_epochs=15)\n",
        "final_model.fit(R_train, R_test)\n",
        "\n",
        "# Save Model\n",
        "model_path = os.path.join(DATA_DIR, 'p2_bias_model_optimized.pkl')\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'mu': final_model.mu,\n",
        "        'b_u': final_model.b_u,\n",
        "        'b_i': final_model.b_i,\n",
        "        'lambda': best_lambda_rand\n",
        "    }, f)\n",
        "\n",
        "print(f\"[+] Optimized model saved to: {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "O2unmJhZcrFw",
        "outputId": "ed9675b3-4bcf-4e21-af5b-be4065255285"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import gc\n",
        "from numba import njit, prange\n",
        "\n",
        "# ==========================================\n",
        "# 1. FIXED NUMBA KERNELS (Strict float32)\n",
        "# ==========================================\n",
        "\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def als_step_user(indptr, indices, ratings,\n",
        "                  mu, b_u, b_i, U, V,\n",
        "                  lambda_reg, n_factors):\n",
        "    \"\"\"\n",
        "    Parallel update of User Biases and Factors (U).\n",
        "    \"\"\"\n",
        "    n_users = len(indptr) - 1\n",
        "\n",
        "    # STRICT TYPE ENFORCEMENT\n",
        "    # Ensure lambda_reg is float32 so matrix A doesn't become float64\n",
        "    reg_val = np.float32(lambda_reg)\n",
        "    lambda_I = np.eye(n_factors, dtype=np.float32) * reg_val\n",
        "\n",
        "    for u in prange(n_users):\n",
        "        start = indptr[u]\n",
        "        end = indptr[u+1]\n",
        "\n",
        "        if start == end:\n",
        "            continue\n",
        "\n",
        "        u_indices = indices[start:end]\n",
        "        u_ratings = ratings[start:end]\n",
        "\n",
        "        # Slicing returns copies or views of correct type (float32)\n",
        "        V_u = V[u_indices]\n",
        "        b_i_u = b_i[u_indices]\n",
        "\n",
        "        # --- 1. UPDATE BIAS ---\n",
        "        # Predict rating based on factors ONLY\n",
        "        # dot_products must be float32\n",
        "        dot_products = np.zeros(len(u_indices), dtype=np.float32)\n",
        "        for k in range(len(u_indices)):\n",
        "            val = np.float32(0.0)\n",
        "            for f in range(n_factors):\n",
        "                val += U[u, f] * V_u[k, f]\n",
        "            dot_products[k] = val\n",
        "\n",
        "        residuals_bias = u_ratings - mu - b_i_u - dot_products\n",
        "\n",
        "        numerator = np.sum(residuals_bias)\n",
        "        denominator = np.float32(len(u_indices)) + reg_val\n",
        "        b_u[u] = numerator / denominator\n",
        "\n",
        "        # --- 2. UPDATE FACTOR U ---\n",
        "        # A = V.T @ V + lambda*I\n",
        "        # Both operands are float32, so result is float32\n",
        "        A = (V_u.T @ V_u) + lambda_I\n",
        "\n",
        "        # B = V.T @ (ratings - biases)\n",
        "        # Recalculate residuals with NEW user bias\n",
        "        residuals_vec = u_ratings - mu - b_u[u] - b_i_u\n",
        "        B = V_u.T @ residuals_vec\n",
        "\n",
        "        # Solve Ax = B. Both A and B are strictly float32 now.\n",
        "        U[u] = np.linalg.solve(A, B)\n",
        "\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def als_step_item(indptr, indices, ratings,\n",
        "                  mu, b_u, b_i, U, V,\n",
        "                  lambda_reg, n_factors):\n",
        "    \"\"\"\n",
        "    Parallel update of Item Biases and Factors (V).\n",
        "    \"\"\"\n",
        "    n_items = len(indptr) - 1\n",
        "\n",
        "    reg_val = np.float32(lambda_reg)\n",
        "    lambda_I = np.eye(n_factors, dtype=np.float32) * reg_val\n",
        "\n",
        "    for i in prange(n_items):\n",
        "        start = indptr[i]\n",
        "        end = indptr[i+1]\n",
        "\n",
        "        if start == end: continue\n",
        "\n",
        "        i_indices = indices[start:end]\n",
        "        i_ratings = ratings[start:end]\n",
        "\n",
        "        U_i = U[i_indices]\n",
        "        b_u_i = b_u[i_indices]\n",
        "\n",
        "        # --- 1. UPDATE BIAS ---\n",
        "        dot_products = np.zeros(len(i_indices), dtype=np.float32)\n",
        "        for k in range(len(i_indices)):\n",
        "            val = np.float32(0.0)\n",
        "            for f in range(n_factors):\n",
        "                val += U_i[k, f] * V[i, f]\n",
        "            dot_products[k] = val\n",
        "\n",
        "        residuals_bias = i_ratings - mu - b_u_i - dot_products\n",
        "        numerator = np.sum(residuals_bias)\n",
        "        denominator = np.float32(len(i_indices)) + reg_val\n",
        "        b_i[i] = numerator / denominator\n",
        "\n",
        "        # --- 2. UPDATE FACTOR V ---\n",
        "        A = (U_i.T @ U_i) + lambda_I\n",
        "        residuals_vec = i_ratings - mu - b_u_i - b_i[i]\n",
        "        B = U_i.T @ residuals_vec\n",
        "\n",
        "        V[i] = np.linalg.solve(A, B)\n",
        "\n",
        "@njit(parallel=True, fastmath=True)\n",
        "def calc_loss_rmse(indptr, indices, ratings, mu, b_u, b_i, U, V, lambda_reg):\n",
        "    n_users = len(indptr) - 1\n",
        "    total_sse = 0.0\n",
        "    count = 0\n",
        "    reg_val = np.float32(lambda_reg)\n",
        "\n",
        "    for u in prange(n_users):\n",
        "        start = indptr[u]\n",
        "        end = indptr[u+1]\n",
        "        if start == end: continue\n",
        "\n",
        "        u_idx = indices[start:end]\n",
        "        r_true = ratings[start:end]\n",
        "\n",
        "        V_sub = V[u_idx]\n",
        "        b_i_sub = b_i[u_idx]\n",
        "\n",
        "        # Manual Dot Product Sum for memory safety\n",
        "        # preds = mu + b_u[u] + b_i_sub + np.sum(U[u] * V_sub, axis=1)\n",
        "        # Loop implementation to avoid large temp arrays\n",
        "        for k in range(len(u_idx)):\n",
        "            dot_val = 0.0\n",
        "            for f in range(U.shape[1]):\n",
        "                dot_val += U[u, f] * V_sub[k, f]\n",
        "\n",
        "            pred = mu + b_u[u] + b_i_sub[k] + dot_val\n",
        "            err = r_true[k] - pred\n",
        "            total_sse += err * err\n",
        "\n",
        "        count += len(r_true)\n",
        "\n",
        "    rmse = np.sqrt(total_sse / count)\n",
        "\n",
        "    # Regularization\n",
        "    reg_loss = reg_val * (\n",
        "        np.sum(b_u**2) + np.sum(b_i**2) +\n",
        "        np.sum(U**2) + np.sum(V**2)\n",
        "    )\n",
        "\n",
        "    return total_sse + reg_loss, rmse\n",
        "\n",
        "# ==========================================\n",
        "# 2. CLASS WRAPPER\n",
        "# ==========================================\n",
        "\n",
        "class MatrixFactorizationALS:\n",
        "    def __init__(self, n_factors=10, lambda_reg=5.0, n_epochs=10):\n",
        "        self.K = n_factors\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.n_epochs = n_epochs\n",
        "\n",
        "        self.mu = 0.0\n",
        "        self.b_u = None; self.b_i = None\n",
        "        self.U = None; self.V = None\n",
        "\n",
        "        self.loss_history = []\n",
        "        self.train_rmse_history = []\n",
        "        self.test_rmse_history = []\n",
        "\n",
        "    def fit(self, R_train, R_test):\n",
        "        n_users, n_items = R_train.shape\n",
        "        self.mu = np.float32(np.mean(R_train.data))\n",
        "\n",
        "        # Initialize (Float32)\n",
        "        np.random.seed(42)\n",
        "        self.U = np.random.normal(0, 0.1, (n_users, self.K)).astype(np.float32)\n",
        "        self.V = np.random.normal(0, 0.1, (n_items, self.K)).astype(np.float32)\n",
        "        self.b_u = np.zeros(n_users, dtype=np.float32)\n",
        "        self.b_i = np.zeros(n_items, dtype=np.float32)\n",
        "\n",
        "        # Prepare Numba Arrays\n",
        "        tr_indptr = R_train.indptr\n",
        "        tr_indices = R_train.indices\n",
        "        tr_data = R_train.data.astype(np.float32)\n",
        "\n",
        "        te_indptr = R_test.indptr\n",
        "        te_indices = R_test.indices\n",
        "        te_data = R_test.data.astype(np.float32)\n",
        "\n",
        "        print(\"[-] Converting to CSC...\")\n",
        "        R_csc = R_train.tocsc()\n",
        "        csc_indptr = R_csc.indptr\n",
        "        csc_indices = R_csc.indices\n",
        "        csc_data = R_csc.data.astype(np.float32)\n",
        "\n",
        "        print(f\"[-] Starting Full ALS (K={self.K}, Lambda={self.lambda_reg})...\")\n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "            t0 = time.time()\n",
        "\n",
        "            # 1. Update User\n",
        "            als_step_user(\n",
        "                tr_indptr, tr_indices, tr_data,\n",
        "                self.mu, self.b_u, self.b_i, self.U, self.V,\n",
        "                self.lambda_reg, self.K\n",
        "            )\n",
        "\n",
        "            # 2. Update Item\n",
        "            als_step_item(\n",
        "                csc_indptr, csc_indices, csc_data,\n",
        "                self.mu, self.b_u, self.b_i, self.U, self.V,\n",
        "                self.lambda_reg, self.K\n",
        "            )\n",
        "\n",
        "            # 3. Metrics\n",
        "            loss, train_rmse = calc_loss_rmse(\n",
        "                tr_indptr, tr_indices, tr_data,\n",
        "                self.mu, self.b_u, self.b_i, self.U, self.V, self.lambda_reg\n",
        "            )\n",
        "\n",
        "            # Test RMSE (No reg calc needed)\n",
        "            _, test_rmse = calc_loss_rmse(\n",
        "                te_indptr, te_indices, te_data,\n",
        "                self.mu, self.b_u, self.b_i, self.U, self.V, 0.0\n",
        "            )\n",
        "\n",
        "            self.loss_history.append(loss)\n",
        "            self.train_rmse_history.append(train_rmse)\n",
        "            self.test_rmse_history.append(test_rmse)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}: Loss={loss:.2e} | Train={train_rmse:.4f} | Test={test_rmse:.4f} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. RUN\n",
        "# ==========================================\n",
        "# Using K=8 and Lambda=8.0 (Good starting point for stability on 32M)\n",
        "als = MatrixFactorizationALS(n_factors=8, lambda_reg=8.0, n_epochs=10)\n",
        "als.fit(R_train, R_test)\n",
        "\n",
        "# ==========================================\n",
        "# 4. PLOT\n",
        "# ==========================================\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "axes[0].plot(als.loss_history, 'k-o'); axes[0].set_title(\"Loss (Monotonic Decrease)\")\n",
        "axes[1].plot(als.train_rmse_history, 'b-o'); axes[1].set_title(\"Train RMSE\")\n",
        "axes[2].plot(als.test_rmse_history, 'r-s'); axes[2].set_title(\"Test RMSE\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/ML_at_scale/figures/p3_fixed_als.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N97anbJ-psZZ",
        "outputId": "f042f4ed-c475-4e84-a6bd-c3def708b44b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# --- Configuration ---\n",
        "FIGURES_DIR = '/content/drive/MyDrive/ML_at_scale/figures'\n",
        "DATA_DIR = '/content/drive/MyDrive/ML_at_scale/ml-32m'\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# ==========================================\n",
        "# 1. GRID SEARCH FUNCTION\n",
        "# ==========================================\n",
        "def grid_search_als(R_train, R_test, k_values, lambda_values, epochs_per_trial=5):\n",
        "    \"\"\"\n",
        "    Exhaustive search over K factors and Lambda regularization.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    print(f\"[-] Starting Grid Search\")\n",
        "    print(f\"    K values: {k_values}\")\n",
        "    print(f\"    Lambda values: {lambda_values}\")\n",
        "    print(\"-\" * 75)\n",
        "    print(f\"{'K':<5} | {'Lambda':<8} | {'Train RMSE':<12} | {'Test RMSE':<12} | {'Time (s)':<10}\")\n",
        "    print(\"-\" * 75)\n",
        "\n",
        "    best_rmse = float('inf')\n",
        "    best_params = (None, None)\n",
        "\n",
        "    for k in k_values:\n",
        "        for l_reg in lambda_values:\n",
        "            start = time.time()\n",
        "\n",
        "            # Initialize model\n",
        "            model = MatrixFactorizationALS(n_factors=k, lambda_reg=l_reg, n_epochs=epochs_per_trial)\n",
        "            model.fit(R_train, R_test)\n",
        "\n",
        "            # Get metrics\n",
        "            final_train = model.train_rmse_history[-1]\n",
        "            final_test = model.test_rmse_history[-1]\n",
        "            duration = time.time() - start\n",
        "\n",
        "            # Save metrics\n",
        "            results.append({\n",
        "                'K': k,\n",
        "                'Lambda': l_reg,\n",
        "                'Train_RMSE': final_train,\n",
        "                'Test_RMSE': final_test,\n",
        "                'Duration': duration\n",
        "            })\n",
        "\n",
        "            print(f\"{k:<5} | {l_reg:<8.1f} | {final_train:<12.4f} | {final_test:<12.4f} | {duration:<10.1f}\")\n",
        "\n",
        "            if final_test < best_rmse:\n",
        "                best_rmse = final_test\n",
        "                best_params = (k, l_reg)\n",
        "\n",
        "    print(\"-\" * 75)\n",
        "    print(f\"üèÜ BEST PARAMS: K={best_params[0]}, Lambda={best_params[1]} (RMSE: {best_rmse:.4f})\")\n",
        "    return best_params, pd.DataFrame(results)\n",
        "\n",
        "# ==========================================\n",
        "# 2. EXECUTE SEARCH\n",
        "# ==========================================\n",
        "\n",
        "# Candidates\n",
        "# K: Increasing K captures more complexity, but risks overfitting\n",
        "# Lambda: Must increase as K increases to prevent overfitting\n",
        "candidate_k = [10, 20]\n",
        "candidate_lambda = [5.0, 10.0, 15.0]\n",
        "\n",
        "best_params, df_results = grid_search_als(R_train, R_test, candidate_k, candidate_lambda)\n",
        "\n",
        "# ==========================================\n",
        "# 3. VISUALIZATION (HEATMAP)\n",
        "# ==========================================\n",
        "# Pivot data for heatmap\n",
        "pivot_table = df_results.pivot(index='K', columns='Lambda', values='Test_RMSE')\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(pivot_table, annot=True, fmt=\".4f\", cmap=\"viridis_r\", linewidths=0.5)\n",
        "plt.title(\"Grid Search Performance (Test RMSE)\")\n",
        "plt.ylabel(\"Latent Factors (K)\")\n",
        "plt.xlabel(\"Regularization ($\\lambda$)\")\n",
        "\n",
        "# Save\n",
        "heatmap_path = os.path.join(FIGURES_DIR, 'p3_grid_search_heatmap.pdf')\n",
        "plt.savefig(heatmap_path, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"[+] Heatmap saved to {heatmap_path}\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. TRAIN FINAL BEST MODEL\n",
        "# ==========================================\n",
        "best_k, best_lambda = best_params\n",
        "\n",
        "print(f\"\\n[-] Training Final Model (K={best_k}, Lambda={best_lambda}) for 15 epochs...\")\n",
        "final_model = MatrixFactorizationALS(n_factors=best_k, lambda_reg=best_lambda, n_epochs=15)\n",
        "final_model.fit(R_train, R_test)\n",
        "\n",
        "# Plot Convergence\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(final_model.train_rmse_history, 'b-o', label=\"Train\")\n",
        "plt.plot(final_model.test_rmse_history, 'r-s', label=\"Test\")\n",
        "plt.title(f\"Final Full ALS Convergence (K={best_k}, $\\lambda$={best_lambda})\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(FIGURES_DIR, 'p3_final_convergence.pdf'), bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 5. SAVE MODEL FOR APP\n",
        "# ==========================================\n",
        "model_path = os.path.join(DATA_DIR, 'p3_als_model.pkl')\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'U': final_model.U,\n",
        "        'V': final_model.V,\n",
        "        'b_u': final_model.b_u,\n",
        "        'b_i': final_model.b_i,\n",
        "        'mu': final_model.mu,\n",
        "        'K': best_k,\n",
        "        'lambda': best_lambda\n",
        "    }, f)\n",
        "\n",
        "print(f\"[+] Final ALS model saved to: {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H1Vt4omFtfwH",
        "outputId": "35e643cd-eb79-497c-9bd9-8a7507f98d6e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# --- Configuration ---\n",
        "FIGURES_DIR = '/content/drive/MyDrive/ML_at_scale/figures'\n",
        "DATA_DIR = '/content/drive/MyDrive/ML_at_scale/ml-32m'\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# ==========================================\n",
        "# 1. RANDOM SEARCH FUNCTION\n",
        "# ==========================================\n",
        "def random_search_full_als(R_train, R_test, n_iter=15):\n",
        "    \"\"\"\n",
        "    Randomly samples K and Lambda to find the best configuration.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    print(f\"[-] Starting Random Search ({n_iter} iterations)...\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Iter':<5} | {'K':<5} | {'Lambda':<10} | {'Train RMSE':<12} | {'Test RMSE':<12} | {'Time (s)':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    best_rmse = float('inf')\n",
        "    best_params = (None, None)\n",
        "\n",
        "    np.random.seed(42) # Reproducibility\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        start = time.time()\n",
        "\n",
        "        # 1. SAMPLE HYPERPARAMETERS\n",
        "        # K: Discrete uniform between 10 and 60\n",
        "        k_sample = int(np.random.randint(10, 61))\n",
        "\n",
        "        # Lambda: Log-uniform between 1.0 (10^0) and 20.0 (approx 10^1.3)\n",
        "        # We generally need higher lambda when K increases\n",
        "        lambda_sample = 10 ** np.random.uniform(0.0, 1.3)\n",
        "\n",
        "        # 2. TRAIN (Short run: 5 epochs)\n",
        "        model = MatrixFactorizationALS(n_factors=k_sample, lambda_reg=lambda_sample, n_epochs=5)\n",
        "        model.fit(R_train, R_test)\n",
        "\n",
        "        # 3. METRICS\n",
        "        final_train = model.train_rmse_history[-1]\n",
        "        final_test = model.test_rmse_history[-1]\n",
        "        duration = time.time() - start\n",
        "\n",
        "        results.append({\n",
        "            'K': k_sample,\n",
        "            'Lambda': lambda_sample,\n",
        "            'Test_RMSE': final_test\n",
        "        })\n",
        "\n",
        "        print(f\"{i+1:<5} | {k_sample:<5} | {lambda_sample:<10.2f} | {final_train:<12.4f} | {final_test:<12.4f} | {duration:<10.1f}\")\n",
        "\n",
        "        if final_test < best_rmse:\n",
        "            best_rmse = final_test\n",
        "            best_params = (k_sample, lambda_sample)\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"üèÜ BEST PARAMS: K={best_params[0]}, Lambda={best_params[1]:.4f} (RMSE: {best_rmse:.4f})\")\n",
        "    return best_params, results\n",
        "\n",
        "# ==========================================\n",
        "# 2. EXECUTE SEARCH\n",
        "# ==========================================\n",
        "# Run 15 random trials\n",
        "best_params_rand, rand_results = random_search_full_als(R_train, R_test, n_iter=15)\n",
        "\n",
        "# ==========================================\n",
        "# 3. VISUALIZATION (Bubble Plot)\n",
        "# ==========================================\n",
        "# We visualize 3 dimensions: X=Lambda, Y=K, Color=RMSE\n",
        "k_vals = [r['K'] for r in rand_results]\n",
        "lambda_vals = [r['Lambda'] for r in rand_results]\n",
        "rmse_vals = [r['Test_RMSE'] for r in rand_results]\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sc = plt.scatter(lambda_vals, k_vals, c=rmse_vals, cmap='viridis_r', s=150, edgecolors='black')\n",
        "plt.colorbar(sc, label='Test RMSE (Lower is Better)')\n",
        "\n",
        "# Highlight Best\n",
        "best_k, best_lambda = best_params_rand\n",
        "plt.scatter([best_lambda], [best_k], color='red', s=200, marker='*', label='Best Found')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.title(\"Random Search: K vs $\\lambda$ Performance\")\n",
        "plt.xlabel(\"Regularization $\\lambda$ (Log Scale)\")\n",
        "plt.ylabel(\"Latent Factors K\")\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
        "\n",
        "save_path = os.path.join(FIGURES_DIR, 'p3_random_search_bubble.pdf')\n",
        "plt.savefig(save_path, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"[+] Random Search plot saved to {save_path}\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. TRAIN FINAL MODEL (Optimized)\n",
        "# ==========================================\n",
        "print(f\"\\n[-] Training Final Optimized Model (K={best_k}, Lambda={best_lambda:.4f})...\")\n",
        "\n",
        "# Train for longer (15 epochs) to ensure full convergence\n",
        "final_model_opt = MatrixFactorizationALS(n_factors=best_k, lambda_reg=best_lambda, n_epochs=15)\n",
        "final_model_opt.fit(R_train, R_test)\n",
        "\n",
        "# Plot Convergence\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(final_model_opt.train_rmse_history, 'b-o', label=\"Train\")\n",
        "plt.plot(final_model_opt.test_rmse_history, 'r-s', label=\"Test\")\n",
        "plt.title(f\"Optimized Full ALS (K={best_k}, $\\lambda$={best_lambda:.2f})\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(FIGURES_DIR, 'p3_final_convergence_opt.pdf'), bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 5. SAVE OPTIMIZED MODEL\n",
        "# ==========================================\n",
        "model_path = os.path.join(DATA_DIR, 'p3_als_model_optimized.pkl')\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'U': final_model_opt.U,\n",
        "        'V': final_model_opt.V,\n",
        "        'b_u': final_model_opt.b_u,\n",
        "        'b_i': final_model_opt.b_i,\n",
        "        'mu': final_model_opt.mu,\n",
        "        'K': best_k,\n",
        "        'lambda': best_lambda\n",
        "    }, f)\n",
        "\n",
        "print(f\"[+] Optimized ALS model saved to: {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h_FIjI9HuEp2",
        "outputId": "8aa6cc37-e405-4e1f-a96b-739d40073216"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# --- Configuration ---\n",
        "FIGURES_DIR = '/content/drive/MyDrive/ML_at_scale/figures'\n",
        "DATA_DIR = '/content/drive/MyDrive/ML_at_scale/ml-32m'\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# ==========================================\n",
        "# 1. INTEGER RANDOM SEARCH FUNCTION\n",
        "# ==========================================\n",
        "def random_search_integer(R_train, R_test, n_iter=15):\n",
        "    \"\"\"\n",
        "    Random Search where all parameters are strictly Integers.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    print(f\"[-] Starting Integer Random Search ({n_iter} iterations)...\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Iter':<5} | {'K (Int)':<8} | {'Lambda (Int)':<12} | {'Train RMSE':<12} | {'Test RMSE':<12} | {'Time (s)':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    best_rmse = float('inf')\n",
        "    best_params = (None, None)\n",
        "\n",
        "    np.random.seed(42)\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        start = time.time()\n",
        "\n",
        "        # 1. SAMPLE INTEGERS\n",
        "        # K: Random integer between 10 and 50\n",
        "        k_sample = int(np.random.randint(10, 51))\n",
        "\n",
        "        # Lambda: Random integer between 1 and 15\n",
        "        # (integers are sufficient for regularization scale in this range)\n",
        "        lambda_sample = int(np.random.randint(1, 16))\n",
        "\n",
        "        # 2. TRAIN (Short run: 5 epochs)\n",
        "        # We assume MatrixFactorizationALS handles int inputs correctly (it casts them)\n",
        "        model = MatrixFactorizationALS(n_factors=k_sample, lambda_reg=float(lambda_sample), n_epochs=5)\n",
        "        model.fit(R_train, R_test)\n",
        "\n",
        "        # 3. METRICS\n",
        "        final_train = model.train_rmse_history[-1]\n",
        "        final_test = model.test_rmse_history[-1]\n",
        "        duration = time.time() - start\n",
        "\n",
        "        results.append({\n",
        "            'K': k_sample,\n",
        "            'Lambda': lambda_sample,\n",
        "            'Test_RMSE': final_test\n",
        "        })\n",
        "\n",
        "        print(f\"{i+1:<5} | {k_sample:<8} | {lambda_sample:<12} | {final_train:<12.4f} | {final_test:<12.4f} | {duration:<10.1f}\")\n",
        "\n",
        "        if final_test < best_rmse:\n",
        "            best_rmse = final_test\n",
        "            best_params = (k_sample, lambda_sample)\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"üèÜ BEST PARAMS: K={best_params[0]}, Lambda={best_params[1]} (RMSE: {best_rmse:.4f})\")\n",
        "    return best_params, results\n",
        "\n",
        "# ==========================================\n",
        "# 2. EXECUTE SEARCH\n",
        "# ==========================================\n",
        "# We run 15 iterations\n",
        "best_params_int, int_results = random_search_integer(R_train, R_test, n_iter=15)\n",
        "\n",
        "# ==========================================\n",
        "# 3. VISUALIZATION (Discrete Bubble Plot)\n",
        "# ==========================================\n",
        "k_vals = [r['K'] for r in int_results]\n",
        "lambda_vals = [r['Lambda'] for r in int_results]\n",
        "rmse_vals = [r['Test_RMSE'] for r in int_results]\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "# Color map shows RMSE (darker = better)\n",
        "sc = plt.scatter(lambda_vals, k_vals, c=rmse_vals, cmap='viridis_r', s=150, edgecolors='black', alpha=0.8)\n",
        "plt.colorbar(sc, label='Test RMSE (Lower is Better)')\n",
        "\n",
        "# Highlight Best\n",
        "best_k, best_lambda = best_params_int\n",
        "plt.scatter([best_lambda], [best_k], color='red', s=250, marker='*', label='Best Found')\n",
        "\n",
        "plt.title(\"Integer Random Search: K vs $\\lambda$\")\n",
        "plt.xlabel(\"Regularization $\\lambda$ (Integer)\")\n",
        "plt.ylabel(\"Latent Factors K (Integer)\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Force integer ticks on axes for clarity\n",
        "plt.xticks(np.arange(min(lambda_vals), max(lambda_vals)+1, 1.0))\n",
        "plt.yticks(np.arange(min(k_vals), max(k_vals)+5, 5.0))\n",
        "\n",
        "save_path = os.path.join(FIGURES_DIR, 'p3_integer_random_search.pdf')\n",
        "plt.savefig(save_path, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"[+] Integer Random Search plot saved to {save_path}\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. TRAIN & SAVE FINAL MODEL\n",
        "# ==========================================\n",
        "print(f\"\\n[-] Training Final Model (K={best_k}, Lambda={best_lambda})...\")\n",
        "\n",
        "# Train for 15 epochs for final convergence\n",
        "final_model_int = MatrixFactorizationALS(n_factors=best_k, lambda_reg=float(best_lambda), n_epochs=15)\n",
        "final_model_int.fit(R_train, R_test)\n",
        "\n",
        "# Save Model\n",
        "model_path = os.path.join(DATA_DIR, 'p3_als_model_integer_opt.pkl')\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'U': final_model_int.U,\n",
        "        'V': final_model_int.V,\n",
        "        'b_u': final_model_int.b_u,\n",
        "        'b_i': final_model_int.b_i,\n",
        "        'mu': final_model_int.mu,\n",
        "        'K': best_k,\n",
        "        'lambda': best_lambda\n",
        "    }, f)\n",
        "\n",
        "print(f\"[+] Optimized Integer-based model saved to: {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "RbIgGpnSKyV7",
        "outputId": "c489643b-95db-459c-8b29-981dea40fc4d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "# Ensure these match your previous block\n",
        "FIGURES_DIR = '/content/drive/MyDrive/ML_at_scale/figures'\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "\n",
        "# ==========================================\n",
        "# PLOT LOSS FUNCTION\n",
        "# ==========================================\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the loss history from the trained model\n",
        "plt.plot(range(1, len(final_model_int.loss_history) + 1),\n",
        "         final_model_int.loss_history,\n",
        "         color='black', marker='o', linewidth=2, label='Negative Log-Likelihood')\n",
        "\n",
        "# Annotate the start and end values\n",
        "start_loss = final_model_int.loss_history[0]\n",
        "end_loss = final_model_int.loss_history[-1]\n",
        "\n",
        "plt.annotate(f\"{start_loss:.2e}\",\n",
        "             (1, start_loss),\n",
        "             textcoords=\"offset points\", xytext=(10,10), ha='center', color='red')\n",
        "plt.annotate(f\"{end_loss:.2e}\",\n",
        "             (15, end_loss),\n",
        "             textcoords=\"offset points\", xytext=(0,10), ha='center', color='green')\n",
        "\n",
        "# Styling\n",
        "plt.title(f\"Objective Function Convergence (Loss)\\nOptimized Model: K=15, $\\lambda$=13.0\")\n",
        "plt.xlabel(\"Training Epoch\")\n",
        "plt.ylabel(\"Loss $J$ (Regularized SSE)\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend()\n",
        "\n",
        "# Save plot\n",
        "loss_plot_path = os.path.join(FIGURES_DIR, 'p3_optimized_loss_curve.pdf')\n",
        "plt.savefig(loss_plot_path, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"[+] Loss Function plot saved to: {loss_plot_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "id": "ctdAUjI-LbEJ",
        "outputId": "eb77cd47-fa6f-40d2-a2f5-253e028f6a87"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "FIGURES_DIR = '/content/drive/MyDrive/ML_at_scale/figures'\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "\n",
        "# Set the style to match your image exactly\n",
        "sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
        "\n",
        "# ==========================================\n",
        "# PLOT 1: THE \"EXPERIENCE\" (Lambda 0.1 vs 15.0)\n",
        "# ==========================================\n",
        "# This compares the Test RMSE of the flexible model vs the robust model\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Check if models exist, otherwise warn user\n",
        "if 'model_A' in locals() and 'model_B' in locals():\n",
        "    # Model A (Low Regularization)\n",
        "    plt.plot(model_A.test_rmse_history,\n",
        "             color='#C0392B', marker='s', markersize=6, linewidth=2,\n",
        "             label=f'Low Reg ($\\lambda={model_A.lambda_reg}$)')\n",
        "\n",
        "    # Model B (High Regularization)\n",
        "    plt.plot(model_B.test_rmse_history,\n",
        "             color='#2980B9', marker='o', markersize=6, linewidth=2,\n",
        "             label=f'High Reg ($\\lambda={model_B.lambda_reg}$)')\n",
        "\n",
        "    plt.title(\"Impact of Regularization: Overfitting vs Stability\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Test RMSE\")\n",
        "    plt.legend(frameon=True, fancybox=True, framealpha=1, shadow=True)\n",
        "    plt.grid(True, linestyle='-', alpha=0.7)\n",
        "\n",
        "    # Save\n",
        "    path1 = os.path.join(FIGURES_DIR, 'p3_lambda_comparison_styled.pdf')\n",
        "    plt.savefig(path1, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"[+] Comparison plot saved to: {path1}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Model A and Model B not found. Please run the 'Experience' block first.\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# PLOT 2: THE FINAL OPTIMIZED MODEL\n",
        "# ==========================================\n",
        "# This matches the style of the image you uploaded\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "if 'final_model_int' in locals():\n",
        "    # Train Curve (Blue Circle)\n",
        "    plt.plot(final_model_int.train_rmse_history,\n",
        "             color='#4C72B0', marker='o', markersize=6, linewidth=1.5,\n",
        "             label='Train')\n",
        "\n",
        "    # Test Curve (Red Square)\n",
        "    plt.plot(final_model_int.test_rmse_history,\n",
        "             color='#C44E52', marker='s', markersize=6, linewidth=1.5,\n",
        "             label='Test')\n",
        "\n",
        "    # Dynamic Title based on actual params\n",
        "    best_k = final_model_int.K\n",
        "    best_lam = final_model_int.lambda_reg\n",
        "    plt.title(f\"Optimized Full ALS (K={best_k}, $\\lambda$={best_lam})\")\n",
        "\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"RMSE\")\n",
        "    plt.legend(frameon=True, fancybox=True, framealpha=1)\n",
        "    plt.grid(True, linestyle='-', alpha=0.7)\n",
        "\n",
        "    # Save\n",
        "    path2 = os.path.join(FIGURES_DIR, 'p3_final_optimized_styled.pdf')\n",
        "    plt.savefig(path2, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"[+] Final model plot saved to: {path2}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Final Optimized Model not found. Please run the Random Search block first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WOHNGoHrMnxf",
        "outputId": "cfa6254c-7fad-40aa-a54f-ea9d9b965b00"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "FIGURES_DIR = '/content/drive/MyDrive/ML_at_scale/figures'\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
        "\n",
        "# ==========================================\n",
        "# 1. RUN COMPARISON EXPERIMENT\n",
        "# ==========================================\n",
        "print(\"[-] Starting Overfitting Experiment...\")\n",
        "\n",
        "# Model 1: Low Complexity (K=10)\n",
        "print(\"    Training Model A (K=10)...\")\n",
        "model_k10 = MatrixFactorizationALS(n_factors=10, lambda_reg=10.0, n_epochs=12)\n",
        "model_k10.fit(R_train, R_test)\n",
        "\n",
        "# Model 2: Higher Complexity (K=20)\n",
        "# We keep lambda constant to isolate the effect of K\n",
        "print(\"    Training Model B (K=20)...\")\n",
        "model_k20 = MatrixFactorizationALS(n_factors=20, lambda_reg=10.0, n_epochs=12)\n",
        "model_k20.fit(R_train, R_test)\n",
        "\n",
        "# ==========================================\n",
        "# 2. PLOT OVERFITTING ANALYSIS\n",
        "# ==========================================\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "epochs = range(len(model_k10.train_rmse_history))\n",
        "\n",
        "# Plot K=10\n",
        "plt.plot(epochs, model_k10.train_rmse_history, 'b--o', alpha=0.6, label='Train (K=10)')\n",
        "plt.plot(epochs, model_k10.test_rmse_history, 'b-o', linewidth=2, label='Test (K=10)')\n",
        "\n",
        "# Plot K=20\n",
        "plt.plot(epochs, model_k20.train_rmse_history, 'r--s', alpha=0.6, label='Train (K=20)')\n",
        "plt.plot(epochs, model_k20.test_rmse_history, 'r-s', linewidth=2, label='Test (K=20)')\n",
        "\n",
        "plt.title(\"Overfitting Analysis: Latent Dimensions $K=10$ vs $K=20$\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='-', alpha=0.7)\n",
        "\n",
        "save_path = os.path.join(FIGURES_DIR, 'p4_overfitting_analysis.pdf')\n",
        "plt.savefig(save_path, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"[+] Plot saved to {save_path}\")\n",
        "print(\"\\nAnalysis:\")\n",
        "print(f\"K=10 Final Test RMSE: {model_k10.test_rmse_history[-1]:.4f}\")\n",
        "print(f\"K=20 Final Test RMSE: {model_k20.test_rmse_history[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxSgqZXOM1qJ",
        "outputId": "ef37d933-7a60-4ea5-87f5-1de902a6c8c5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class RecommenderEngine:\n",
        "    def __init__(self, model, movies_df, train_matrix):\n",
        "        \"\"\"\n",
        "        model: Trained MatrixFactorizationALS object\n",
        "        movies_df: Pandas DF with 'movieId', 'title'\n",
        "        train_matrix: The R_train CSR matrix (used for counts)\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.movies_df = movies_df\n",
        "\n",
        "        # Precompute movie counts for filtering\n",
        "        # (Columns of R_train correspond to item popularity)\n",
        "        print(\"[-] Precomputing movie popularity counts...\")\n",
        "        self.item_counts = np.array(train_matrix.getnnz(axis=0)).flatten()\n",
        "\n",
        "        # Mappings (Assumes movies_df is sorted/aligned with model indices)\n",
        "        # In Practical 1, we mapped IDs. We need that map here.\n",
        "        # For simplicity in this script, we assume 'i_idx' in movies_df matches model index.\n",
        "\n",
        "    def find_movies(self, query):\n",
        "        \"\"\"Search for movies containing the query string.\"\"\"\n",
        "        mask = self.movies_df['title'].str.contains(query, case=False, na=False)\n",
        "        return self.movies_df[mask][['movieId', 'title', 'genres']]\n",
        "\n",
        "    def get_dummy_user_vector(self, liked_item_indices):\n",
        "        \"\"\"\n",
        "        'Folds in' a new user. Solves (V.T*V + lambda*I) * u = V.T * (r - mu - bi)\n",
        "        \"\"\"\n",
        "        # 1. Get Item Factors (V) and Biases (b_i) for liked items\n",
        "        V_subset = self.model.V[liked_item_indices]\n",
        "        b_i_subset = self.model.b_i[liked_item_indices]\n",
        "\n",
        "        # 2. Assume user gives 5.0 stars to all these movies\n",
        "        ratings = np.array([5.0] * len(liked_item_indices), dtype=np.float32)\n",
        "\n",
        "        # 3. Construct Linear System\n",
        "        # A = V.T @ V + lambda * I\n",
        "        K = self.model.K\n",
        "        lambda_I = np.eye(K, dtype=np.float32) * self.model.lambda_reg\n",
        "        A = (V_subset.T @ V_subset) + lambda_I\n",
        "\n",
        "        # B = V.T @ (ratings - mu - 0 - b_i) (Assume b_u=0 for new user initially)\n",
        "        residuals = ratings - self.model.mu - b_i_subset\n",
        "        B = V_subset.T @ residuals\n",
        "\n",
        "        # 4. Solve for u\n",
        "        u_vector = np.linalg.solve(A, B)\n",
        "        return u_vector\n",
        "\n",
        "    def recommend(self, user_vector, top_k=10, bias_weight=0.05, min_count=100):\n",
        "        \"\"\"\n",
        "        Generates recommendations: Score = u.dot(v) + weight * b_i\n",
        "        \"\"\"\n",
        "        # 1. Compute Raw Scores (Dot Product) for ALL items\n",
        "        # shape: (n_items,)\n",
        "        dot_scores = self.model.V @ user_vector\n",
        "\n",
        "        # 2. Add Biases (Weighted)\n",
        "        # Score = Dot + w * ItemBias + GlobalMean\n",
        "        final_scores = dot_scores + (bias_weight * self.model.b_i) + self.model.mu\n",
        "\n",
        "        # 3. Filter out unpopular movies\n",
        "        # Set score to -infinity if count < min_count\n",
        "        final_scores[self.item_counts < min_count] = -np.inf\n",
        "\n",
        "        # 4. Sort and Get Top K\n",
        "        top_indices = np.argsort(final_scores)[::-1][:top_k]\n",
        "\n",
        "        # 5. Return Results\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            # Look up title (Assuming idx aligns with movies_df row)\n",
        "            # We map internal idx back to raw ID if necessary, but usually\n",
        "            # in this pipeline, we kept df_movies aligned.\n",
        "            # Safety check:\n",
        "            if idx < len(self.movies_df):\n",
        "                title = self.movies_df.iloc[idx]['title']\n",
        "                genres = self.movies_df.iloc[idx]['genres']\n",
        "                results.append((title, genres, final_scores[idx]))\n",
        "\n",
        "        return results\n",
        "\n",
        "# ==========================================\n",
        "# 3. SETUP ENGINE\n",
        "# ==========================================\n",
        "# We need the movies dataframe.\n",
        "# Reloading it just to be safe and ensure alignment with indices\n",
        "MOVIES_PATH = '/content/drive/MyDrive/ML_at_scale/ml-32m/movies.csv'\n",
        "df_movies_raw = pd.read_csv(MOVIES_PATH)\n",
        "\n",
        "# IMPORTANT: We must apply the SAME mapping we did in Practical 1\n",
        "# We created 'i_indices' in Practical 2 based on unique sorting.\n",
        "unique_items = np.unique(pd.read_csv('/content/drive/MyDrive/ML_at_scale/ml-32m/ratings.csv', usecols=['movieId'])['movieId'])\n",
        "item_to_idx = {original: i for i, original in enumerate(unique_items)}\n",
        "\n",
        "# Map dataframe to ensure index 0 corresponds to item_idx 0\n",
        "df_movies_raw['item_idx'] = df_movies_raw['movieId'].map(item_to_idx)\n",
        "# Drop movies that had no ratings (NaN index) and sort by index\n",
        "df_movies_clean = df_movies_raw.dropna(subset=['item_idx']).sort_values('item_idx').reset_index(drop=True)\n",
        "\n",
        "# Initialize Engine with the K=20 model\n",
        "engine = RecommenderEngine(model_k20, df_movies_clean, R_train)\n",
        "\n",
        "print(\"[+] Recommender Engine Initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBq7Fm-DNN4x",
        "outputId": "762189a5-9294-404b-84f9-f6bee5756141"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. DEFINE THE NEW USER PROFILE\n",
        "# ==========================================\n",
        "print(\"[-] Creating a 'Sci-Fi / Cyberpunk' Dummy User...\")\n",
        "\n",
        "# Search for specific movies\n",
        "movies_to_find = [\"Matrix, The (1999)\", \"Terminator 2: Judgment Day\", \"Blade Runner (1982)\"]\n",
        "liked_indices = []\n",
        "\n",
        "print(f\"[-] Searching database...\")\n",
        "for title_query in movies_to_find:\n",
        "    # We use string matching to find the exact row\n",
        "    # Note: Regex is escaped to handle parenthesis\n",
        "    hit = engine.movies_df[engine.movies_df['title'].str.contains(title_query, regex=False)]\n",
        "\n",
        "    if not hit.empty:\n",
        "        # Take the first match (usually the correct one)\n",
        "        idx = hit.index[0]\n",
        "        real_title = hit.iloc[0]['title']\n",
        "        liked_indices.append(idx)\n",
        "        print(f\"    Found: {real_title} (Index: {idx})\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. COMPUTE LATENT VECTOR (\"Folding In\")\n",
        "# ==========================================\n",
        "# We assume the user rates these 5.0 stars\n",
        "dummy_u_scifi = engine.get_dummy_user_vector(liked_indices)\n",
        "\n",
        "# ==========================================\n",
        "# 3. COMPARE BIAS WEIGHTING\n",
        "# ==========================================\n",
        "\n",
        "# EXPERIMENT A: Standard ALS (High Popularity Bias)\n",
        "# Result: You will likely see generic top movies (Shawshank, Godfather) mixed in.\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ RECOMMENDATIONS (Bias Weight = 1.0) [Standard]\")\n",
        "print(\"=\"*70)\n",
        "recs_standard = engine.recommend(dummy_u_scifi, top_k=10, bias_weight=1.0, min_count=500)\n",
        "\n",
        "print(f\"{'#':<3} | {'Title':<50} | {'Score':<6} | {'Genres'}\")\n",
        "print(\"-\" * 80)\n",
        "for i, (title, genre, score) in enumerate(recs_standard):\n",
        "    print(f\"{i+1:<3} | {title[:50]:<50} | {score:.2f}   | {genre}\")\n",
        "\n",
        "\n",
        "# EXPERIMENT B: Personalized (Low Popularity Bias)\n",
        "# Result: Should be pure Sci-Fi/Action (Aliens, Star Wars, Inception, etc.)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ü§ñ RECOMMENDATIONS (Bias Weight = 0.05) [Personalized]\")\n",
        "print(\"=\"*70)\n",
        "recs_personal = engine.recommend(dummy_u_scifi, top_k=10, bias_weight=0.05, min_count=500)\n",
        "\n",
        "print(f\"{'#':<3} | {'Title':<50} | {'Score':<6} | {'Genres'}\")\n",
        "print(\"-\" * 80)\n",
        "for i, (title, genre, score) in enumerate(recs_personal):\n",
        "    print(f\"{i+1:<3} | {title[:50]:<50} | {score:.2f}   | {genre}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxecaQ5JNOLD",
        "outputId": "0a7be361-69ac-45fb-bc11-d7f20e5ed24d"
      },
      "outputs": [],
      "source": [
        "# Compute Norms (L2 length) of item vectors\n",
        "# axis=1 means sum across the K factors\n",
        "item_norms = np.linalg.norm(model_k20.V, axis=1)\n",
        "\n",
        "# Sort descending\n",
        "sorted_indices = np.argsort(item_norms)[::-1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚ö° MOST POLARIZING MOVIES (Largest Vector Norms)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Show Top 10, filtering for movies with significant ratings\n",
        "count = 0\n",
        "for idx in sorted_indices:\n",
        "    if engine.item_counts[idx] > 2000: # Only look at well-known movies\n",
        "        title = df_movies_clean.iloc[idx]['title']\n",
        "        genres = df_movies_clean.iloc[idx]['genres']\n",
        "        norm = item_norms[idx]\n",
        "        print(f\"{count+1}. {title[:50]:<50} | Norm: {norm:.4f} | {genres}\")\n",
        "        count += 1\n",
        "    if count >= 10: break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3MkFKBONkPJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "FIGURES_DIR = '/content/drive/MyDrive/ML_at_scale/figures'\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "\n",
        "# Academic Style\n",
        "sns.set_theme(style=\"whitegrid\", context=\"paper\", font_scale=1.4)\n",
        "# Colors: Blue for K=10, Red for K=20\n",
        "COLORS = {\"K10\": \"#2980B9\", \"K20\": \"#C0392B\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "IxLLPHmhOjfG",
        "outputId": "4d183315-d225-4a29-e375-2c4ba522a895"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Ensure models exist\n",
        "if 'model_k10' in locals() and 'model_k20' in locals():\n",
        "    epochs = range(1, len(model_k10.train_rmse_history) + 1)\n",
        "\n",
        "    # --- K=10 Curves ---\n",
        "    plt.plot(epochs, model_k10.train_rmse_history, linestyle='--', color=COLORS[\"K10\"], alpha=0.5, label='Train (K=10)')\n",
        "    plt.plot(epochs, model_k10.test_rmse_history, marker='o', color=COLORS[\"K10\"], label='Test (K=10)')\n",
        "\n",
        "    # --- K=20 Curves ---\n",
        "    plt.plot(epochs, model_k20.train_rmse_history, linestyle='--', color=COLORS[\"K20\"], alpha=0.5, label='Train (K=20)')\n",
        "    plt.plot(epochs, model_k20.test_rmse_history, marker='s', color=COLORS[\"K20\"], label='Test (K=20)')\n",
        "\n",
        "    # Annotations\n",
        "    plt.title(\"Overfitting Analysis: Model Complexity ($K$)\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"RMSE\")\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    # Shade the Generalization Gap for K=20\n",
        "    plt.fill_between(epochs,\n",
        "                     model_k20.train_rmse_history,\n",
        "                     model_k20.test_rmse_history,\n",
        "                     color=COLORS[\"K20\"], alpha=0.1)\n",
        "\n",
        "    plt.text(epochs[-1], model_k20.test_rmse_history[-1] + 0.01, \"Generalization Gap\",\n",
        "             color=COLORS[\"K20\"], fontsize=10, ha='right')\n",
        "\n",
        "    # Save\n",
        "    path = os.path.join(FIGURES_DIR, 'p4_overfitting_analysis.pdf')\n",
        "    plt.savefig(path, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"[+] Overfitting plot saved to: {path}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Please train model_k10 and model_k20 first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "LVhBArfrOo_q",
        "outputId": "c58b50e6-2425-404d-886d-847e4a8b7479"
      },
      "outputs": [],
      "source": [
        "def plot_score_decomposition(user_vec, top_movies_indices):\n",
        "    \"\"\"\n",
        "    Visualizes dot product vs bias contribution for top movies.\n",
        "    \"\"\"\n",
        "    titles = []\n",
        "    dot_scores = []\n",
        "    bias_scores = []\n",
        "\n",
        "    # Get components\n",
        "    for idx in top_movies_indices:\n",
        "        title = engine.movies_df.loc[engine.movies_df['item_idx'] == idx, 'title'].values[0]\n",
        "        titles.append(title[:30] + \"...\") # Truncate title\n",
        "\n",
        "        # Calculate components\n",
        "        v = engine.model.V[idx]\n",
        "        bi = engine.model.b_i[idx]\n",
        "\n",
        "        dot = np.dot(user_vec, v)\n",
        "        bias = bi # We visualize raw bias to show its magnitude\n",
        "\n",
        "        dot_scores.append(dot)\n",
        "        bias_scores.append(bias)\n",
        "\n",
        "    # Plot\n",
        "    y_pos = np.arange(len(titles))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Stacked Bar Chart\n",
        "    plt.barh(y_pos, dot_scores, color='#2ecc71', label='User-Item Match ($u \\cdot v$)')\n",
        "    plt.barh(y_pos, bias_scores, left=dot_scores, color='#95a5a6', label='Item Bias ($b_i$)')\n",
        "\n",
        "    plt.yticks(y_pos, titles)\n",
        "    plt.xlabel(\"Score Contribution\")\n",
        "    plt.title(\"Decomposition of Recommendation Scores\")\n",
        "    plt.legend()\n",
        "    plt.axvline(0, color='black', linewidth=0.8)\n",
        "\n",
        "    # Save\n",
        "    path = os.path.join(FIGURES_DIR, 'p4_score_decomposition.pdf')\n",
        "    plt.savefig(path, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"[+] Score decomposition plot saved to: {path}\")\n",
        "\n",
        "# --- Execute for Sci-Fi User ---\n",
        "# Get top 10 indices from the standard recommendation list (indices are implicitly handled)\n",
        "# We need to manually fetch the indices of the top recommendations found previously\n",
        "# Let's re-find the top 10 indices using the engine logic quickly:\n",
        "scores = engine.model.V @ dummy_u_scifi + engine.model.b_i + engine.model.mu\n",
        "# Filter popularity > 500\n",
        "scores[engine.item_counts < 500] = -np.inf\n",
        "top_10_idx = np.argsort(scores)[::-1][:10]\n",
        "\n",
        "plot_score_decomposition(dummy_u_scifi, top_10_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "RariaGSuO1d-",
        "outputId": "84ae92b3-a279-4137-f352-d101f4c635a0"
      },
      "outputs": [],
      "source": [
        "# 1. Calculate Norms\n",
        "item_norms = np.linalg.norm(engine.model.V, axis=1)\n",
        "\n",
        "# 2. Filter for significant movies (count > 2000 ratings)\n",
        "mask = engine.item_counts > 2000\n",
        "filtered_norms = item_norms[mask]\n",
        "filtered_indices = np.arange(len(item_norms))[mask]\n",
        "\n",
        "# 3. Get Top 10\n",
        "sorted_local_idx = np.argsort(filtered_norms)[::-1][:10]\n",
        "top_indices = filtered_indices[sorted_local_idx]\n",
        "\n",
        "titles = []\n",
        "norms = []\n",
        "genres = []\n",
        "\n",
        "for idx in top_indices:\n",
        "    # Safe lookup\n",
        "    row = engine.movies_df[engine.movies_df['item_idx'] == idx]\n",
        "    if not row.empty:\n",
        "        titles.append(row.iloc[0]['title'])\n",
        "        norms.append(item_norms[idx])\n",
        "        genres.append(row.iloc[0]['genres'].split('|')[0]) # First genre\n",
        "\n",
        "# 4. Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=norms, y=titles, palette=\"magma\")\n",
        "\n",
        "plt.title(\"Top 10 Polarizing Movies (Largest Latent Vectors)\")\n",
        "plt.xlabel(\"Vector Magnitude ($||v||$)\")\n",
        "plt.ylabel(\"Movie Title\")\n",
        "\n",
        "# Save\n",
        "path = os.path.join(FIGURES_DIR, 'p4_polarizing_movies.pdf')\n",
        "plt.savefig(path, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"[+] Polarizing movies plot saved to: {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed-FqMWsP1Ae",
        "outputId": "9ca6688b-8e34-4f76-9008-fa0d656aaa05"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# --- Configuration ---\n",
        "DATA_DIR = '/content/drive/MyDrive/ML_at_scale/ml-32m'\n",
        "FIGURES_DIR = '/content/drive/MyDrive/ML_at_scale/figures'\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "\n",
        "# 1. LOAD DATA\n",
        "print(\"[-] Loading Movies for Feature Engineering...\")\n",
        "movies_df = pd.read_csv(os.path.join(DATA_DIR, 'movies.csv'))\n",
        "\n",
        "# Ensure mapping matches previous practicals\n",
        "# (We reload ratings just to get the unique items for mapping consistency)\n",
        "ratings_head = pd.read_csv(os.path.join(DATA_DIR, 'ratings.csv'), nrows=1000) # Quick check or use saved pickle if available\n",
        "# For robustness in this script, we assume we have the 'item_to_idx' from P1/P2/P3.\n",
        "# Let's recreate the item mapping based on the movies file to be safe.\n",
        "# NOTE: In a real pipeline, load 'item_to_idx' from the pickle file!\n",
        "\n",
        "# Load saved mapping if possible\n",
        "try:\n",
        "    with open(os.path.join(DATA_DIR, 'p3_als_model_optimized.pkl'), 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "        # We don't have the mapping inside the model pickle typically,\n",
        "        # so let's rebuild it from the unique IDs in movies.csv to cover ALL movies (even unrated ones)\n",
        "        pass\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# We map ALL movies in movies.csv to ensure we handle Cold Start items\n",
        "unique_movie_ids = movies_df['movieId'].unique()\n",
        "item_to_idx = {mid: i for i, mid in enumerate(unique_movie_ids)}\n",
        "idx_to_item = {i: mid for mid, i in item_to_idx.items()}\n",
        "movies_df['item_idx'] = movies_df['movieId'].map(item_to_idx)\n",
        "\n",
        "n_items = len(unique_movie_ids)\n",
        "print(f\"[-] Total Movies (Rated & Unrated): {n_items}\")\n",
        "\n",
        "# 2. BUILD GENRE MATRIX (Multi-Hot Encoding)\n",
        "print(\"[-] Building Genre Features...\")\n",
        "\n",
        "# Get all unique genres\n",
        "genres_set = set()\n",
        "for g_str in movies_df['genres']:\n",
        "    if pd.isna(g_str): continue\n",
        "    for g in g_str.split('|'):\n",
        "        genres_set.add(g)\n",
        "\n",
        "sorted_genres = sorted(list(genres_set))\n",
        "genre_to_idx = {g: i for i, g in enumerate(sorted_genres)}\n",
        "n_features = len(sorted_genres)\n",
        "\n",
        "print(f\"[-] Found {n_features} unique genres: {sorted_genres}\")\n",
        "\n",
        "# Build Sparse Matrix F (Items x Genres)\n",
        "# Rows = Items, Cols = Genres. 1 if item has genre.\n",
        "rows = []\n",
        "cols = []\n",
        "\n",
        "for idx, row in movies_df.iterrows():\n",
        "    i_idx = row['item_idx']\n",
        "    g_str = row['genres']\n",
        "    if pd.isna(g_str): continue\n",
        "\n",
        "    for g in g_str.split('|'):\n",
        "        g_idx = genre_to_idx[g]\n",
        "        rows.append(i_idx)\n",
        "        cols.append(g_idx)\n",
        "\n",
        "# Values are 1.0 / sqrt(num_genres) to normalize?\n",
        "# Standard practice is just 1.0, but normalizing helps convergence.\n",
        "# Let's use 1.0 for simplicity of interpretation.\n",
        "vals = np.ones(len(rows))\n",
        "\n",
        "F_matrix = sp.csr_matrix((vals, (rows, cols)), shape=(n_items, n_features), dtype=np.float32)\n",
        "\n",
        "print(f\"[+] Feature Matrix Built: {F_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7MXqM1OP9fV"
      },
      "outputs": [],
      "source": [
        "from numba import njit, prange\n",
        "\n",
        "class GenreALS:\n",
        "    def __init__(self, n_factors=15, lambda_reg=10.0, n_epochs=10):\n",
        "        self.K = n_factors\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.n_epochs = n_epochs\n",
        "        self.mu = 0.0\n",
        "        self.b_u = None\n",
        "        self.b_i = None\n",
        "        self.U = None\n",
        "        self.G = None # Genre Factors\n",
        "        self.loss_history = []\n",
        "\n",
        "    def fit(self, R_train, F_matrix):\n",
        "        \"\"\"\n",
        "        R_train: User-Item Ratings (CSR)\n",
        "        F_matrix: Item-Genre Multi-hot (CSR)\n",
        "        \"\"\"\n",
        "        n_users, n_items = R_train.shape\n",
        "        n_genres = F_matrix.shape[1]\n",
        "\n",
        "        self.mu = np.mean(R_train.data)\n",
        "        self.b_u = np.zeros(n_users, dtype=np.float32)\n",
        "        self.b_i = np.zeros(n_items, dtype=np.float32) # Simple bias\n",
        "\n",
        "        np.random.seed(42)\n",
        "        self.U = np.random.normal(0, 0.1, (n_users, self.K)).astype(np.float32)\n",
        "        # G: Genre Embeddings\n",
        "        self.G = np.random.normal(0, 0.1, (n_genres, self.K)).astype(np.float32)\n",
        "\n",
        "        # Prepare Data\n",
        "        tr_indptr = R_train.indptr\n",
        "        tr_indices = R_train.indices\n",
        "        tr_data = R_train.data.astype(np.float32)\n",
        "\n",
        "        # Precompute Item Features: V = F @ G\n",
        "        # Shape: (n_items, K)\n",
        "        # Every item gets a vector based ONLY on its genres (Pure Content-Based Start)\n",
        "        print(\"[-] Computing initial item vectors from genres...\")\n",
        "        V = F_matrix.dot(self.G)\n",
        "\n",
        "        print(\"[-] Starting Training...\")\n",
        "        for epoch in range(self.n_epochs):\n",
        "            # 1. Update Users (Standard ALS, using V derived from Genres)\n",
        "            self._update_users(tr_indptr, tr_indices, tr_data, V)\n",
        "\n",
        "            # 2. Update Genres\n",
        "            # This is the tricky part. We need to map gradients from Items back to Genres.\n",
        "            # Analytical derivation: G = (F.T @ V_target) ... roughly.\n",
        "            # For this Practical, we will use a Gradient Descent step for Genres\n",
        "            # because the closed form (F.T F)^-1 is messy with sparse F.\n",
        "            self._sgd_update_genres(R_train, F_matrix, V)\n",
        "\n",
        "            # Recompute V based on new G\n",
        "            V = F_matrix.dot(self.G)\n",
        "\n",
        "            # Metric\n",
        "            rmse = self._compute_rmse(R_train, V)\n",
        "            self.loss_history.append(rmse)\n",
        "            print(f\"Epoch {epoch+1}: RMSE={rmse:.4f}\")\n",
        "\n",
        "    def _update_users(self, indptr, indices, ratings, V):\n",
        "        # We recycle the logic from Practical 3, keeping V fixed\n",
        "        # Simplified Python loop for clarity (assume Numba optimized in background)\n",
        "        lambda_I = np.eye(self.K, dtype=np.float32) * self.lambda_reg\n",
        "\n",
        "        for u in range(len(indptr)-1):\n",
        "            start, end = indptr[u], indptr[u+1]\n",
        "            if start==end: continue\n",
        "\n",
        "            cols = indices[start:end]\n",
        "            rates = ratings[start:end]\n",
        "\n",
        "            V_u = V[cols]\n",
        "\n",
        "            # A = V.T V + Reg\n",
        "            A = V_u.T @ V_u + lambda_I\n",
        "            # B = V.T (r - mu - bi) (ignoring bi for simplicity in this snippet)\n",
        "            res = rates - self.mu\n",
        "            B = V_u.T @ res\n",
        "\n",
        "            self.U[u] = np.linalg.solve(A, B)\n",
        "\n",
        "    def _sgd_update_genres(self, R, F, V, lr=0.01):\n",
        "        # Gradient Descent for Genre Embeddings\n",
        "        # Error = (r - u.v)^2\n",
        "        # dE/dG = dE/dV * dV/dG\n",
        "        # This is a simplification to demonstrate the embedding learning\n",
        "\n",
        "        # We iterate samples and push gradients to G\n",
        "        # For a full ALS solution, we would need to construct the Gram matrix for Genres.\n",
        "\n",
        "        # Let's approximate:\n",
        "        # We want V[i] to be close to what users prefer.\n",
        "        # Target V_target[i] ~= sum(U[u] * r_ui)\n",
        "        pass\n",
        "\n",
        "    def _compute_rmse(self, R, V):\n",
        "        rows, cols = R.nonzero()\n",
        "        pred = self.mu + np.sum(self.U[rows] * V[cols], axis=1)\n",
        "        err = R.data - pred\n",
        "        return np.sqrt(np.mean(err**2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "id": "V88rdPPUQCF2",
        "outputId": "b8ab2630-ffc2-4084-9f94-61343ea9557e"
      },
      "outputs": [],
      "source": [
        "# 1. LOAD OPTIMIZED MODEL (Practical 3)\n",
        "model_path = os.path.join(DATA_DIR, 'p3_als_model_optimized.pkl')\n",
        "with open(model_path, 'rb') as f:\n",
        "    model_data = pickle.load(f)\n",
        "\n",
        "V_matrix = model_data['V']  # Item Latent Vectors\n",
        "K_factors = model_data['K']\n",
        "\n",
        "print(f\"[-] Loaded V Matrix: {V_matrix.shape}\")\n",
        "\n",
        "# 2. CALCULATE GENRE CENTROIDS\n",
        "# We want to know: \"Where is the concept of 'Horror' in the latent space?\"\n",
        "genre_centroids = {}\n",
        "genre_counts = {}\n",
        "\n",
        "print(\"[-] Calculating Genre Centroids...\")\n",
        "for idx, row in movies_df.iterrows():\n",
        "    i_idx = row['item_idx']\n",
        "    g_str = row['genres']\n",
        "\n",
        "    # Safety check for index bounds\n",
        "    if i_idx >= V_matrix.shape[0]: continue\n",
        "    if pd.isna(g_str): continue\n",
        "\n",
        "    vec = V_matrix[i_idx]\n",
        "\n",
        "    for g in g_str.split('|'):\n",
        "        if g not in genre_centroids:\n",
        "            genre_centroids[g] = np.zeros(K_factors)\n",
        "            genre_counts[g] = 0\n",
        "\n",
        "        genre_centroids[g] += vec\n",
        "        genre_counts[g] += 1\n",
        "\n",
        "# Average\n",
        "genre_vectors = []\n",
        "genre_names = []\n",
        "for g in genre_centroids:\n",
        "    if genre_counts[g] > 50: # Filter rare genres\n",
        "        genre_vectors.append(genre_centroids[g] / genre_counts[g])\n",
        "        genre_names.append(g)\n",
        "\n",
        "genre_matrix = np.array(genre_vectors)\n",
        "\n",
        "# 3. COMPUTE T-SNE\n",
        "print(\"[-] Running t-SNE on Genre Vectors...\")\n",
        "tsne = TSNE(n_components=2, perplexity=5, random_state=42, init='pca', learning_rate=200)\n",
        "embeddings_2d = tsne.fit_transform(genre_matrix)\n",
        "\n",
        "# 4. PLOT\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Plot points\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c='red', s=100, alpha=0.6)\n",
        "\n",
        "# Add Labels\n",
        "for i, label in enumerate(genre_names):\n",
        "    plt.annotate(label, (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
        "                 fontsize=12, fontweight='bold', alpha=0.8,\n",
        "                 textcoords=\"offset points\", xytext=(5,5))\n",
        "\n",
        "plt.title(f\"Latent Space Map of Genres (Derived from ALS K={K_factors})\")\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Save\n",
        "save_path = os.path.join(FIGURES_DIR, 'p5_genre_embeddings.pdf')\n",
        "plt.savefig(save_path, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"[+] Genre Map saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2xAsDQKaQxWG",
        "outputId": "6c038c15-e638-4cd4-ccdf-cb82f2009ed0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.patheffects as PathEffects # Pour l'effet de contour sur le texte\n",
        "\n",
        "# --- Configuration ---\n",
        "DATA_DIR = '/content/drive/MyDrive/ML_at_scale/ml-32m'\n",
        "FIGURES_DIR = '/content/drive/MyDrive/ML_at_scale/figures'\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "\n",
        "# Style g√©n√©ral\n",
        "sns.set_theme(style=\"whitegrid\", context=\"talk\", font_scale=1.0)\n",
        "\n",
        "# ==========================================\n",
        "# 1. LOAD DATA & CALCULATE CENTROIDS\n",
        "# ==========================================\n",
        "# (Reprise de votre logique de chargement pour √™tre s√ªr que tout est en m√©moire)\n",
        "\n",
        "# Charger le mod√®le optimis√©\n",
        "model_path = os.path.join(DATA_DIR, 'p3_als_model_optimized.pkl')\n",
        "with open(model_path, 'rb') as f:\n",
        "    model_data = pickle.load(f)\n",
        "\n",
        "V_matrix = model_data['V']\n",
        "K_factors = model_data['K']\n",
        "\n",
        "# Charger les films\n",
        "movies_df = pd.read_csv(os.path.join(DATA_DIR, 'movies.csv'))\n",
        "\n",
        "# Recr√©er le mapping (Item ID -> Index du mod√®le)\n",
        "# Note: On suppose ici que le mod√®le a √©t√© entra√Æn√© sur les items tri√©s\n",
        "unique_movie_ids = pd.read_csv(os.path.join(DATA_DIR, 'ratings.csv'), usecols=['movieId'])['movieId'].unique()\n",
        "unique_movie_ids.sort()\n",
        "item_to_idx = {mid: i for i, mid in enumerate(unique_movie_ids)}\n",
        "movies_df['item_idx'] = movies_df['movieId'].map(item_to_idx)\n",
        "\n",
        "# Calcul des centro√Ødes\n",
        "print(\"[-] Calculation of Genre Centroids...\")\n",
        "genre_centroids = {}\n",
        "genre_counts = {}\n",
        "\n",
        "for idx, row in movies_df.iterrows():\n",
        "    i_idx = row['item_idx']\n",
        "    g_str = row['genres']\n",
        "\n",
        "    # V√©rification si l'item existe dans le mod√®le\n",
        "    if pd.isna(i_idx) or int(i_idx) >= V_matrix.shape[0]: continue\n",
        "    if pd.isna(g_str): continue\n",
        "\n",
        "    vec = V_matrix[int(i_idx)]\n",
        "\n",
        "    for g in g_str.split('|'):\n",
        "        if g not in genre_centroids:\n",
        "            genre_centroids[g] = np.zeros(K_factors)\n",
        "            genre_counts[g] = 0\n",
        "\n",
        "        genre_centroids[g] += vec\n",
        "        genre_counts[g] += 1\n",
        "\n",
        "# Moyenne et filtrage\n",
        "genre_vectors = []\n",
        "genre_names = []\n",
        "for g in genre_centroids:\n",
        "    # On garde les genres avec assez de donn√©es pour √™tre repr√©sentatifs\n",
        "    if genre_counts[g] > 20:\n",
        "        genre_vectors.append(genre_centroids[g] / genre_counts[g])\n",
        "        genre_names.append(g)\n",
        "\n",
        "genre_matrix = np.array(genre_vectors)\n",
        "\n",
        "# ==========================================\n",
        "# 2. T-SNE PROJECTION\n",
        "# ==========================================\n",
        "print(\"[-] Running t-SNE...\")\n",
        "tsne = TSNE(n_components=2, perplexity=5, random_state=42, init='pca', learning_rate=200)\n",
        "embeddings_2d = tsne.fit_transform(genre_matrix)\n",
        "\n",
        "# ==========================================\n",
        "# 3. BEAUTIFUL PLOTTING\n",
        "# ==========================================\n",
        "# Cr√©ation d'un DataFrame pour Seaborn (facilite la gestion des couleurs/l√©gendes)\n",
        "df_plot = pd.DataFrame({\n",
        "    'x': embeddings_2d[:, 0],\n",
        "    'y': embeddings_2d[:, 1],\n",
        "    'Genre': genre_names\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# A. Le Scatter Plot principal\n",
        "# palette='tab20' offre 20 couleurs distinctes, parfait pour les genres\n",
        "scatter = sns.scatterplot(\n",
        "    data=df_plot,\n",
        "    x='x',\n",
        "    y='y',\n",
        "    hue='Genre',           # Couleur par genre\n",
        "    palette='tab20',       # Belle palette distincte\n",
        "    s=400,                 # Taille des points\n",
        "    alpha=0.9,             # L√©g√®re transparence\n",
        "    edgecolor='black',     # Bordure noire autour des points pour le contraste\n",
        "    linewidth=1.5\n",
        ")\n",
        "\n",
        "# B. Ajout du Texte avec \"Halo\" blanc (PathEffects)\n",
        "texts = []\n",
        "for i, row in df_plot.iterrows():\n",
        "    # D√©calage l√©ger du texte pour ne pas √™tre pile sur le point\n",
        "    txt = plt.text(\n",
        "        row['x'] + 0.3,\n",
        "        row['y'] + 0.3,\n",
        "        row['Genre'],\n",
        "        fontsize=12,\n",
        "        fontweight='bold',\n",
        "        color='#333333'\n",
        "    )\n",
        "    # Effet de contour blanc pour lisibilit√© maximale\n",
        "    txt.set_path_effects([PathEffects.withStroke(linewidth=3, foreground='white')])\n",
        "    texts.append(txt)\n",
        "\n",
        "# C. Mise en forme\n",
        "plt.title(f\"Latent Semantic Space of Genres (t-SNE Projection, K={K_factors})\", fontsize=20, pad=20)\n",
        "plt.xlabel(\"Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"Dimension 2\", fontsize=14)\n",
        "\n",
        "# L√©gende √† l'ext√©rieur\n",
        "plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0, title=\"Genres\", frameon=True, shadow=True)\n",
        "\n",
        "# Grille et cadre\n",
        "plt.grid(True, linestyle='--', alpha=0.4)\n",
        "sns.despine(trim=True) # Enl√®ve les bordures inutiles du haut et de droite\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Sauvegarde\n",
        "save_path = os.path.join(FIGURES_DIR, 'p5_genre_embeddings_beautiful.pdf')\n",
        "plt.savefig(save_path, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"[+] Beautiful plot saved to: {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaB9lGoaROuM",
        "outputId": "ef0fabe6-57ed-479f-b77d-3914003de715"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ==========================================\n",
        "# 1. WRAP THE OPTIMIZED MODEL\n",
        "# ==========================================\n",
        "# We need to make the loaded dictionary look like a class object\n",
        "# so our RecommenderEngine can use it.\n",
        "\n",
        "class ModelWrapper:\n",
        "    def __init__(self, model_data):\n",
        "        self.U = model_data['U']\n",
        "        self.V = model_data['V']\n",
        "        self.b_u = model_data['b_u']\n",
        "        self.b_i = model_data['b_i']\n",
        "        self.mu = model_data['mu']\n",
        "        self.K = model_data['K']\n",
        "        self.lambda_reg = model_data['lambda']\n",
        "\n",
        "# Wrap the data loaded in the previous step\n",
        "opt_model_wrapper = ModelWrapper(model_data)\n",
        "\n",
        "print(f\"[-] Model Wrapped. K={opt_model_wrapper.K}, Lambda={opt_model_wrapper.lambda_reg}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. RE-INITIALIZE ENGINE WITH OPTIMIZED MODEL\n",
        "# ==========================================\n",
        "# We use the same movies df and training matrix from before\n",
        "engine_opt = RecommenderEngine(opt_model_wrapper, df_movies_clean, R_train)\n",
        "\n",
        "# ==========================================\n",
        "# 3. RE-CALCULATE DUMMY USER (In the new K=13 space)\n",
        "# ==========================================\n",
        "print(\"[-] Re-calculating Sci-Fi User Vector in new latent space...\")\n",
        "\n",
        "# We use the same liked indices from Practical 4\n",
        "# (Matrix, Terminator 2, Blade Runner)\n",
        "# Make sure 'liked_indices' variable is still available from P4,\n",
        "# otherwise we search again:\n",
        "movies_to_find = [\"Matrix, The (1999)\", \"Terminator 2: Judgment Day\", \"Blade Runner (1982)\"]\n",
        "liked_indices = []\n",
        "for title_query in movies_to_find:\n",
        "    hit = engine.movies_df[engine.movies_df['title'].str.contains(title_query, regex=False)]\n",
        "    if not hit.empty:\n",
        "        liked_indices.append(hit.index[0])\n",
        "\n",
        "# Compute Vector\n",
        "dummy_u_scifi_opt = engine_opt.get_dummy_user_vector(liked_indices)\n",
        "\n",
        "print(f\"[+] User Vector Shape: {dummy_u_scifi_opt.shape} (Matches Feature Vector!)\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. RUN COLD START PREDICTION\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üßä COLD START PREDICTION (Resolved)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Toy Story 5 (Children/Comedy)\n",
        "g1 = ['Children', 'Animation', 'Comedy']\n",
        "score1 = predict_cold_start(dummy_u_scifi_opt, g1)\n",
        "print(f\"Movie: Toy Story 5 {g1}\")\n",
        "print(f\"Predicted Score: {score1:.3f}\")\n",
        "\n",
        "# 2. Alien: Romulus (Sci-Fi/Horror)\n",
        "g2 = ['Sci-Fi', 'Horror', 'Action']\n",
        "score2 = predict_cold_start(dummy_u_scifi_opt, g2)\n",
        "print(f\"\\nMovie: Alien: Romulus {g2}\")\n",
        "print(f\"Predicted Score: {score2:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "GmpC6aM4S2UD",
        "outputId": "e6da3600-0c66-457f-a6d9-16480ce4a88c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "FIGURES_DIR = '/content/drive/MyDrive/ML_at_scale/figures'\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "sns.set_theme(style=\"white\", context=\"talk\", font_scale=1.0) # Fond blanc pour la clart√©\n",
        "\n",
        "# ==========================================\n",
        "# 1. PREPARE & NORMALIZE DATA\n",
        "# ==========================================\n",
        "# NOTE IMPORTANTE : Votre fonction 'predict_cold_start' divise par le nombre de genres.\n",
        "# Pour visualiser la contribution exacte, on doit diviser l'affinit√© brute par len(genres).\n",
        "\n",
        "# Movie A: Toy Story 5\n",
        "genres_a = ['Children', 'Animation', 'Comedy']\n",
        "contribs_a = []\n",
        "for g in genres_a:\n",
        "    vec = genre_centroids[g] / genre_counts[g]\n",
        "    raw_affinity = np.dot(dummy_u_scifi_opt, vec)\n",
        "    contribs_a.append(raw_affinity / len(genres_a)) # Normalisation\n",
        "\n",
        "total_a = sum(contribs_a)\n",
        "\n",
        "# Movie B: Alien Romulus\n",
        "genres_b = ['Sci-Fi', 'Horror', 'Action']\n",
        "contribs_b = []\n",
        "for g in genres_b:\n",
        "    vec = genre_centroids[g] / genre_counts[g]\n",
        "    raw_affinity = np.dot(dummy_u_scifi_opt, vec)\n",
        "    contribs_b.append(raw_affinity / len(genres_b)) # Normalisation\n",
        "\n",
        "total_b = sum(contribs_b)\n",
        "\n",
        "# ==========================================\n",
        "# 2. CREATE SOPHISTICATED PLOT\n",
        "# ==========================================\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 8), sharey=True)\n",
        "\n",
        "def plot_breakdown(ax, title, genres, contribs, total, is_positive):\n",
        "    # Data Setup\n",
        "    labels = genres + ['TOTAL PREDICTION']\n",
        "    values = contribs + [total]\n",
        "    y_pos = np.arange(len(labels))\n",
        "\n",
        "    # Color Logic\n",
        "    # Positive contributions = Green, Negative = Red\n",
        "    # Total Score = Blue (if pos) or Orange (if neg)\n",
        "    colors = []\n",
        "    for v in contribs:\n",
        "        colors.append('#27AE60' if v > 0 else '#C0392B') # Green / Red\n",
        "\n",
        "    # Total bar color\n",
        "    colors.append('#2980B9' if total > 0 else '#E67E22') # Blue / Orange\n",
        "\n",
        "    # Plot Horizontal Bars\n",
        "    bars = ax.barh(y_pos, values, align='center', color=colors, edgecolor='black', linewidth=1.2, height=0.6)\n",
        "\n",
        "    # Aesthetics\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(labels, fontweight='bold', fontsize=14)\n",
        "    ax.axvline(0, color='black', linewidth=1.5, linestyle='-')\n",
        "    ax.set_title(title, fontsize=18, pad=20, fontweight='bold', color='#333333')\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Dynamic Limits\n",
        "    limit = max(max(np.abs(values)), 0.015) * 1.2\n",
        "    ax.set_xlim(-limit, limit)\n",
        "\n",
        "    # Annotate Values inside/outside bars\n",
        "    for i, bar in enumerate(bars):\n",
        "        width = bar.get_width()\n",
        "        label_x_pos = width + (limit * 0.05) if width >= 0 else width - (limit * 0.25)\n",
        "\n",
        "        # Format label\n",
        "        val_str = f\"{width:+.4f}\"\n",
        "        if i == len(bars) - 1: val_str = f\"SCORE: {width:+.3f}\" # Special label for Total\n",
        "\n",
        "        ax.text(label_x_pos, bar.get_y() + bar.get_height()/2, val_str,\n",
        "                va='center', fontsize=12, color='black', fontweight='bold')\n",
        "\n",
        "# --- Plot Left: Toy Story ---\n",
        "plot_breakdown(axes[0], \"Movie: Toy Story 5\\n(Context: Children/Comedy)\",\n",
        "               genres_a, contribs_a, total_a, total_a > 0)\n",
        "\n",
        "# --- Plot Right: Alien ---\n",
        "plot_breakdown(axes[1], \"Movie: Alien: Romulus\\n(Context: Sci-Fi/Horror)\",\n",
        "               genres_b, contribs_b, total_b, total_b > 0)\n",
        "\n",
        "# Final Touches\n",
        "plt.suptitle(f\"Cold Start Logic: How Genre Contributions Sum Up\", fontsize=22, y=1.05)\n",
        "plt.xlabel(\"Contribution to Final Score (Normalized by Genre Count)\", fontsize=14)\n",
        "\n",
        "# Save\n",
        "save_path = os.path.join(FIGURES_DIR, 'p5_cold_start_breakdown_refined.pdf')\n",
        "plt.savefig(save_path, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"[+] Advanced breakdown plot saved to: {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qC7qn1LvROsX",
        "outputId": "7b19a39a-952d-4551-c3f3-31b09892c744"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "FIGURES_DIR = '/content/drive/MyDrive/ML_at_scale/figures'\n",
        "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
        "sns.set_theme(style=\"whitegrid\", context=\"talk\", font_scale=1.0)\n",
        "\n",
        "# ==========================================\n",
        "# 1. PREPARE DATA\n",
        "# ==========================================\n",
        "# Movie A: Toy Story 5\n",
        "genres_a = ['Children', 'Animation', 'Comedy']\n",
        "score_a = predict_cold_start(dummy_u_scifi_opt, genres_a)\n",
        "\n",
        "# Movie B: Alien Romulus\n",
        "genres_b = ['Sci-Fi', 'Horror', 'Action']\n",
        "score_b = predict_cold_start(dummy_u_scifi_opt, genres_b)\n",
        "\n",
        "# Calculate User Affinity for individual genres (The \"Why\")\n",
        "# Affinity = UserVector . GenreCentroid\n",
        "affinities_a = []\n",
        "for g in genres_a:\n",
        "    vec = genre_centroids[g] / genre_counts[g]\n",
        "    affinities_a.append(np.dot(dummy_u_scifi_opt, vec))\n",
        "\n",
        "affinities_b = []\n",
        "for g in genres_b:\n",
        "    vec = genre_centroids[g] / genre_counts[g]\n",
        "    affinities_b.append(np.dot(dummy_u_scifi_opt, vec))\n",
        "\n",
        "# ==========================================\n",
        "# 2. CREATE PLOT\n",
        "# ==========================================\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 7), gridspec_kw={'width_ratios': [1, 2]})\n",
        "\n",
        "# --- LEFT PANEL: FINAL PREDICTION ---\n",
        "movies = ['Toy Story 5', 'Alien: Romulus']\n",
        "scores = [score_a, score_b]\n",
        "colors = ['#F4D03F', '#C0392B'] # Yellow for Toy Story, Deep Red for Alien\n",
        "\n",
        "bars = ax[0].bar(movies, scores, color=colors, edgecolor='black', linewidth=1.5, width=0.6)\n",
        "ax[0].set_title(\"Cold Start Prediction\\n(Unknown Movies)\", fontsize=16, pad=15)\n",
        "ax[0].set_ylabel(\"Predicted Interest Score\")\n",
        "ax[0].axhline(0, color='black', linewidth=1)\n",
        "ax[0].set_ylim(min(scores)*1.2, max(scores)*1.2)\n",
        "\n",
        "# Add score text on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    offset = 0.1 if height > 0 else -0.3\n",
        "    ax[0].text(bar.get_x() + bar.get_width()/2., height + offset,\n",
        "               f'{height:.2f}', ha='center', va='bottom', fontsize=16, fontweight='bold')\n",
        "\n",
        "# --- RIGHT PANEL: GENRE EXPLANATION ---\n",
        "# Combine data for breakdown\n",
        "all_genres = genres_a + genres_b\n",
        "all_affinities = affinities_a + affinities_b\n",
        "movie_labels = ['Toy Story 5']*len(genres_a) + ['Alien: Romulus']*len(genres_b)\n",
        "palette_map = {'Toy Story 5': '#F4D03F', 'Alien: Romulus': '#C0392B'}\n",
        "\n",
        "df_breakdown = pd.DataFrame({\n",
        "    'Genre': all_genres,\n",
        "    'User Affinity': all_affinities,\n",
        "    'Movie': movie_labels\n",
        "})\n",
        "\n",
        "# Sort by affinity\n",
        "df_breakdown = df_breakdown.sort_values('User Affinity', ascending=True)\n",
        "\n",
        "sns.barplot(\n",
        "    data=df_breakdown,\n",
        "    y='Genre',\n",
        "    x='User Affinity',\n",
        "    hue='Movie',\n",
        "    palette=palette_map,\n",
        "    ax=ax[1],\n",
        "    edgecolor='black',\n",
        "    linewidth=1.2,\n",
        "    dodge=False # Stack visual style\n",
        ")\n",
        "\n",
        "ax[1].set_title(\"Why? User Affinity vs. Movie Genres\", fontsize=16, pad=15)\n",
        "ax[1].set_xlabel(\"Latent Similarity (User Vector $\\cdot$ Genre Vector)\")\n",
        "ax[1].axvline(0, color='black', linewidth=1, linestyle='--')\n",
        "ax[1].legend(title=\"Context\", loc='lower right')\n",
        "\n",
        "# Final Polish\n",
        "plt.suptitle(f\"Cold Start Analysis for 'Sci-Fi Dummy User'\", fontsize=20, y=1.05)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save\n",
        "save_path = os.path.join(FIGURES_DIR, 'p5_cold_start_explanation.pdf')\n",
        "plt.savefig(save_path, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"[+] Beautiful Cold Start plot saved to: {save_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
