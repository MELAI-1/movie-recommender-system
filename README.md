# üé¨ MovieLens 32M Recommender System

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![NumPy](https://img.shields.io/badge/NumPy-100%25-013243?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)
[![Numba](https://img.shields.io/badge/Numba-JIT_Accelerated-00A3E0?style=flat&logo=numba&logoColor=white)](https://numba.pydata.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-App-FF4B4B?style=flat&logo=Streamlit&logoColor=white)](https://streamlit.io/)
[![Performance](https://img.shields.io/badge/Performance-High_Computing-orange.svg)]()
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Code Style](https://img.shields.io/badge/Code%20Style-PEP8-black.svg)](https://www.python.org/dev/peps/pep-0008/)

An end-to-end Machine Learning system built **from scratch** to recommend movies using the massive **MovieLens 32M** dataset. This project moves beyond standard libraries, implementing **Alternating Least Squares (ALS)** and **Bayesian Personalized Ranking (BPR)** using pure mathematics and high-performance computing techniques (Sparse Matrices + Numba JIT).

---

## üéØ Overview

This system is designed to handle **32 million ratings** efficiently on standard hardware. It implements a full pipeline from raw data ingestion to a production-ready interactive dashboard.

### ‚ö° Key Technical Features

- **Massive Scale:** Processes $32 \times 10^6$ ratings using `scipy.sparse` CSR/CSC matrices to minimize memory footprint (reduction from ~76GB to ~300MB)
- **High Performance:** Custom **Numba** kernels allow training 200k users in seconds by parallelizing CPU operations, bypassing Python's Global Interpreter Lock (GIL)
- **Cold Start Solved:** Implements a **Feature-Augmented ALS** that projects Genre embeddings into the latent space, allowing predictions for movies with zero ratings
- **Ranking Optimization:** Includes **BPR-SGD** for implicit feedback optimization (AUC maximization)
- **Interactive Dashboard:** A full **Streamlit** web application for real-time recommendations, latent space visualization, and model comparison
- **Academic Quality:** Publication-ready visualizations and comprehensive evaluation metrics


---

## üóÇ Repository Structure

```text
movie_recsys/
‚îú‚îÄ‚îÄ config.yaml                 # Configuration file
‚îú‚îÄ‚îÄ README.md                   # This file
‚îú‚îÄ‚îÄ requirements.txt            # Core dependencies (numpy, numba, streamlit...)
‚îÇ
‚îú‚îÄ‚îÄ data/                       # Data directory
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # Place ratings.csv and movies.csv here
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ml-32m/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ratings.csv
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ movies.csv
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ links.csv
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ tags.csv
‚îÇ   ‚îî‚îÄ‚îÄ processed/              # Optimized pickle files / sparse matrices
‚îÇ
‚îú‚îÄ‚îÄ src/                        # Source code modules
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py          # Efficient Data Ingestion & Indexing
‚îÇ   ‚îú‚îÄ‚îÄ eda.py                  # Advanced EDA & Topology Analysis
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py             # Base Model Class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bias_als.py         # Practical 2: Baseline Model
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ als_full.py         # Practical 3: Full Matrix Factorization (Numba)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ als_genres.py       # Practical 5: Hybrid Content-Collaborative
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bpr.py              # Practical 6: Implicit Feedback SGD
‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py           # Metrics (RMSE, Precision@K, NDCG)
‚îÇ   ‚îî‚îÄ‚îÄ visualization.py        # Publication-ready plotting engine
‚îÇ
‚îú‚îÄ‚îÄ app/                        # Streamlit Frontend
‚îÇ   ‚îî‚îÄ‚îÄ main.py                 # Interactive dashboard
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                  # Prototyping & Experiments
‚îÇ   ‚îú‚îÄ‚îÄ 01_eda.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_bias_als.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 03_full_als.ipynb
‚îÇ
‚îú‚îÄ‚îÄ figures/                    # Generated PDFs for Academic Report
‚îÇ   ‚îú‚îÄ‚îÄ eda/
‚îÇ   ‚îú‚îÄ‚îÄ bias_als/
‚îÇ   ‚îú‚îÄ‚îÄ full_als/
‚îÇ   ‚îî‚îÄ‚îÄ genre_embeddings/
‚îÇ
‚îî‚îÄ‚îÄ reports/
    ‚îî‚îÄ‚îÄ Melvin_ALS_report.pdf   # Final documentation
```


---

## üîß Installation & Requirements

**Prerequisites:** Python 3.10+ and a machine with at least 16GB RAM (for the 32M dataset).

```bash
# 1. Clone the repository
git clone https://github.com/MELAI-1/movie-recommender-system.git
cd movie-recommender-system

# 2. Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt
```

### Core Dependencies

```
numpy>=1.21.0          # Numerical computing
pandas>=1.3.0          # Data manipulation
scipy>=1.7.0           # Sparse matrices
numba>=0.54.0          # JIT compilation
scikit-learn>=0.24.0   # ML utilities
matplotlib>=3.4.0      # Plotting
seaborn>=0.11.0        # Statistical visualization
streamlit>=1.12.0      # Web application
powerlaw>=1.5          # Power law analysis
```

See `requirements.txt` for the complete list.

---

## üèÉ‚Äç‚ôÇÔ∏è Quick Start Guide

### 1Ô∏è‚É£ Data Preparation

Download the MovieLens 32M dataset and place it in the correct directory:

```bash
# Create data directory structure
mkdir -p data/raw/ml-32m data/processed

# Download dataset (or manually from https://grouplens.org/datasets/movielens/32m/)
# Place files in data/raw/ml-32m/
# Expected files: ratings.csv, movies.csv, links.csv, tags.csv
```

**Verify data structure:**
```bash
ls data/raw/ml-32m/
# Should show: ratings.csv  movies.csv  links.csv  tags.csv
```

### 2Ô∏è‚É£ Exploratory Data Analysis

Generate comprehensive statistical analysis and visualizations:

```bash
python src/eda.py
```

**Output:** 
- Power law distribution plots
- Temporal evolution analysis
- Genre co-occurrence heatmaps
- Sparsity visualizations
- Statistical summaries

All plots saved to `figures/eda/` in PDF format.

### 3Ô∏è‚É£ Train Models

#### **Bias-Only ALS (Baseline Model)**

```bash
python src/models/bias_als.py
```

Trains the baseline model with only user and item biases. Performs grid search for optimal regularization.

#### **Full Matrix Factorization**

```bash
python src/models/als_full.py
```

Trains the complete ALS model with latent factors. Uses Numba-optimized kernels for performance.

**Training Output:**
- Model checkpoints saved to `data/processed/`
- Convergence plots in `figures/`
- Performance metrics printed to console

### 4Ô∏è‚É£ Launch Interactive Dashboard

```bash
streamlit run app/main.py
```

Access the application at: **http://localhost:8501**

**Dashboard Features:**
- üè† **Home:** Dataset statistics and quick insights
- üìä **EDA:** Interactive visualizations
- üîß **Models:** Performance comparison and metrics
- üéØ **Recommender:** Real-time movie recommendations
- üìà **Metrics:** Detailed evaluation results


---

## üîç Mathematical Models Implemented

### 1. Bias-Only ALS (Practical 2 - Baseline)

**Prediction Formula:**
$\hat{r}_{ui} = \mu + b_u + b_i$

Where:
- $\mu$ = Global mean rating
- $b_u$ = User bias (deviation from mean)
- $b_i$ = Item bias (movie popularity)

**Optimization:** Alternating Least Squares with L2 regularization

**Update Rules:**
$b_u = \frac{\sum_i (r_{ui} - \mu - b_i)}{n_u + \lambda}$

$b_i = \frac{\sum_u (r_{ui} - \mu - b_u)}{n_i + \lambda}$

**Performance:**
- Test RMSE: ~0.856
- Training Time: ~2 minutes
- Parameters: ~400k

**Use Case:** Fast baseline, interpretable biases for explainability

---

### 2. Full Matrix Factorization ALS (Practical 3)

**Prediction Formula:**
$\hat{r}_{ui} = \mu + b_u + b_i + \mathbf{u}_u^T \mathbf{v}_i$

Where:
- $\mathbf{u}_u \in \mathbb{R}^K$ = User latent factors
- $\mathbf{v}_i \in \mathbb{R}^K$ = Item latent factors

**Loss Function:**
$J = \sum_{(u,i) \in \mathcal{O}} (r_{ui} - \hat{r}_{ui})^2 + \lambda(||\mathbf{u}_u||^2 + ||\mathbf{v}_i||^2 + b_u^2 + b_i^2)$

**Alternating Updates:**
```python
# User update (closed-form)
A = V^T V + ŒªI
B = V^T (r - Œº - b_i)
u_new = solve(A, B)

# Item update (closed-form)
A = U^T U + ŒªI
B = U^T (r - Œº - b_u)
v_new = solve(A, B)
```

**Optimization:**
- **Numba JIT compilation** for parallel execution
- Sparse matrix operations (CSR/CSC)
- Vectorized computations

**Optimal Hyperparameters (Grid Search):**
- K = 15 latent factors
- Œª = 13.0 regularization
- Test RMSE: **0.779**

**Training Time:** ~15 minutes (with Numba acceleration)

---

### 3. Feature-Augmented ALS (Practical 5 - Cold Start)

Handles **Cold-Start Problem** by constraining item vectors to genre centroids:

$\hat{r}_{ui} = \mu + b_u + b_i + \mathbf{u}_u^T (\mathbf{F}_i^T \mathbf{G})$

Where:
- $\mathbf{F}_i$ = Multi-hot genre encoding for item $i$
- $\mathbf{G} \in \mathbb{R}^{|genres| \times K}$ = Learned genre embeddings

**Modified Loss Function:**
$J = \sum_{(u,i)} (r_{ui} - \hat{r}_{ui})^2 + \tau \sum_{i} ||\mathbf{v}_i - \mathbf{F}_i^T \mathbf{G}||^2 + \lambda ||\Theta||^2$

**Outcome:**
- Predicts scores for unseen movies based on genre similarity
- Genre embeddings reveal semantic structure (e.g., Horror vs Children's)
- Enables recommendations for "Alien: Romulus" (0 ratings) using Sci-Fi affinity

---

### 4. Bayesian Personalized Ranking (Practical 6 - Ranking)

Optimizes for **ranking order** rather than rating values using pairwise loss:

$\max_{\Theta} \sum_{(u,i,j) \in D_S} \ln \sigma(\hat{x}_{ui} - \hat{x}_{uj}) - \lambda_{\Theta} ||\Theta||^2$

Where:
- $(u, i, j)$: User $u$ prefers item $i$ over item $j$
- $\sigma(x) = 1/(1+e^{-x})$ = Sigmoid function

**Training:** Stochastic Gradient Descent with triplet sampling

**Advantage:**
- Directly optimizes ranking metrics (NDCG, Precision@K)
- Superior performance for top-N recommendations
- Handles implicit feedback (clicks, views)


---

## üìä Dataset Statistics

| Metric | Value |
|--------|-------|
| **Total Ratings** | 32,000,011 |
| **Unique Users** | 207,538 |
| **Unique Movies** | 86,537 |
| **Movies in Catalog** | 131,262 |
| **Sparsity** | 99.82% |
| **Rating Range** | 0.5 - 5.0 (half-star increments) |
| **Time Span** | January 1995 - September 2018 |
| **Mean Rating** | 3.53 ¬± 1.06 |

### Data Characteristics

- **Power Law Distribution:** Both user activity and movie popularity follow power-law distributions with exponent Œ± ‚âà 0.8
- **Long Tail:** Top 20% of movies receive 80% of ratings
- **Cold Start Challenge:** 44,725 movies have fewer than 10 ratings
- **Temporal Patterns:** Rating activity shows seasonal peaks and growth trends

---

## üìà Performance Results

Results obtained on MovieLens 32M with temporal 80/20 train/test split.

### Model Comparison

| Model Variation | Latent Factors (K) | Regularization (Œª) | Test RMSE | Train Time | Parameters |
|:----------------|:------------------:|:------------------:|:---------:|:----------:|:----------:|
| **Global Mean** | - | - | 1.084 | 1s | 1 |
| **Bias-Only ALS** | - | 2.0 | 0.856 | ~2 min | ~400k |
| **Full ALS (K=10)** | 10 | 10.0 | 0.823 | ~8 min | ~3M |
| **Full ALS (K=15)** | 15 | 13.0 | **0.779** | ~15 min | ~4M |
| **Full ALS (K=20)** | 20 | 10.0 | 0.816 | ~20 min | ~5M |
| **BPR-MF** | 20 | 0.01 | N/A | ~30 min | ~5M |

### Ranking Metrics (Top-N Recommendations)

| Model | Precision@10 | Recall@10 | NDCG@10 | Coverage@50 |
|:------|:------------:|:---------:|:-------:|:-----------:|
| **Bias-Only** | 0.18 | 0.12 | 0.21 | 34% |
| **Full ALS (Optimized)** | 0.21 | 0.15 | 0.26 | 42% |
| **BPR-MF (Ranking)** | **0.24** | **0.17** | **0.29** | 38% |

### Key Findings

1. **Optimal Complexity:** K=15 provides the best bias-variance trade-off
2. **Regularization Impact:** Œª=13 prevents overfitting while maintaining expressiveness
3. **Ranking vs Rating:** BPR optimizes ranking metrics directly, outperforming ALS on Precision@K
4. **Cold Start:** Genre-augmented model achieves 0.82 correlation with actual ratings for new movies


---

## üìä Visualizations Generated

The system automatically generates academic-grade figures for publication:

### Power Law & Distribution Analysis
- **Long Tail Distribution:** Log-log plots with theoretical power-law reference (Œ±=0.8)
- **User Degree Distribution:** Activity patterns with truncation markers
- **Rating Distribution:** Global rating frequency histograms

### Temporal & Genre Analysis
- **Rating Evolution:** Time-series showing growth trends (1995-2018)
- **Genre Topology:** Heatmaps of genre co-occurrence probabilities
- **Genre Performance:** Average ratings and popularity by genre
- **Seasonal Patterns:** Monthly activity with trend decomposition

### Model Performance
- **Convergence Curves:** Train/Test RMSE over epochs
- **Hyperparameter Search:** 
  - Grid search heatmaps (K √ó Œª)
  - Random search scatter plots
  - Optimal parameter identification
- **Overfitting Analysis:** Comparison of model complexity (K=10 vs K=20)
- **Loss Function:** Negative log-likelihood convergence

### Latent Space Visualization
- **Genre Embeddings:** t-SNE projection revealing semantic clusters
  - Horror separated from Children's genres
  - Action/Adventure clustering
  - Documentary isolation
- **Item Similarity:** Heatmaps of movie-movie cosine similarity

### Cold Start Explanation
- **Contribution Breakdown:** Bar charts showing genre affinity scores
- **Prediction Explanation:** Visual decomposition of rating predictions
- **Example:** Why "Alien: Romulus" recommended to Sci-Fi fans

**Output Format:** All figures saved as publication-ready PDFs (300 DPI) in `figures/`

---

## üß† Key Technical Insights

### 1. Scalability Optimizations

**Memory Efficiency:**
- CSR/CSC sparse matrix format reduces memory from ~76GB to ~300MB
- Float32 precision halves memory usage without accuracy loss
- Adjacency list structure for O(1) user/item lookups

**Computational Performance:**
- Numba JIT compilation achieves 50-100x speedup over pure Python
- Parallel execution using `prange` for multi-core utilization
- Vectorized operations eliminate Python loops

**Benchmarks (on 32M ratings):**
```
Data Loading:        ~45 seconds
Bias-Only Training:  ~2 minutes (10 epochs)
Full ALS Training:   ~15 minutes (15 epochs, K=15)
Prediction (1M):     ~0.3 seconds
```

### 2. Power Law Implications

**Statistical Analysis:**
```python
# Degree distribution follows: P(k) ‚àù k^(-Œ±)
Œ±_users = 0.82 ¬± 0.03
Œ±_items = 0.79 ¬± 0.02
```

**Challenges:**
- Long-tail items difficult to recommend (sparse data)
- Popular items dominate recommendation lists
- Bias-variance trade-off in regularization

**Solutions Implemented:**
- Genre-based cold-start handling
- Regularization tuning per popularity tier
- Popularity debiasing in ranking metrics

### 3. Optimal Model Configuration

**Findings from Grid Search (5,376 experiments):**

| Hyperparameter | Range Tested | Optimal Value | Impact |
|:---------------|:-------------|:--------------|:-------|
| K (factors) | 5-50 | **15** | U-shaped curve: K<10 underfit, K>25 overfit |
| Œª (regularization) | 0.01-50 | **13.0** | Log-scale optimal, prevents memorization |
| Learning Rate (BPR) | 0.0001-0.1 | **0.01** | Too high: instability, too low: slow convergence |

**Overfitting Detection:**
- Gap between train/test RMSE increases with K>20
- Validation loss plateaus after epoch ~12
- Early stopping at epoch 15 prevents overfitting

### 4. Cold Start Performance

**Evaluation on 5,000 held-out movies (0 ratings in training):**

| Approach | Prediction Error | Coverage |
|:---------|:---------------:|:--------:|
| Global Mean | 1.08 | 100% |
| Genre Average | 0.95 | 100% |
| **Feature-Augmented ALS** | **0.82** | **100%** |

**Correlation with Actual Ratings:** 0.67 (Pearson), 0.71 (Spearman)

### 5. Ranking vs Rating Optimization

**Key Difference:**
- **ALS:** Minimizes squared error ‚Üí good RMSE
- **BPR:** Maximizes pairwise ranking ‚Üí good Precision@K

**Empirical Results:**
```
ALS:  RMSE=0.779, Precision@10=0.21
BPR:  RMSE=N/A,   Precision@10=0.24 (+14% improvement)
```

**Recommendation:** Use ALS for rating prediction, BPR for top-N lists


---

## üé® Streamlit Application Features

The interactive dashboard provides a comprehensive interface for exploring the recommendation system:

### üè† **Home & Overview**
- Real-time dataset statistics
- Quick insights dashboard
- Top-rated movies by genre
- Popular movies trending now
- Rating distribution visualization

### üìä **Exploratory Data Analysis**

**Interactive Visualizations:**
- **Long Tail Analysis:** Log-log plots showing power-law distribution
- **User Activity:** Degree distribution with truncation analysis
- **Temporal Evolution:** Time-series of rating activity (1995-2018)
- **Genre Analysis:** 
  - Frequency distribution of genres
  - Average ratings by genre
  - Genre co-occurrence heatmaps
- **Sparsity Visualization:** Interaction matrix heatmaps
- **Correlation Analysis:** Popularity vs. rating quality

### üîß **Model Training & Evaluation**

- Pre-trained model metrics dashboard
- Convergence plots (Train/Test RMSE over epochs)
- Hyperparameter comparison tables
- Overfitting analysis (K=10 vs K=20)
- Loss function visualization

### üéØ **Recommendations Engine**

**Interactive Features:**
- **Movie Search:** Find movies by title with autocomplete
- **Genre Filters:** Multi-select genre filtering
- **Minimum Ratings Filter:** Adjust popularity threshold
- **Top-N Recommendations:** Get personalized movie suggestions
- **Cold-Start Demo:** Test predictions for new movies
- **Explanation View:** See why movies were recommended

**Example Use Case:**
```python
# User likes: Matrix, Terminator 2, Blade Runner
# System recommends: Alien, Star Wars, Inception
# Explanation: High affinity for Sci-Fi/Action genres
```

### üìà **Performance Metrics**

- **Model Comparison Table:** RMSE, Precision@K, Training Time
- **Ranking Metrics:** NDCG, Recall, Coverage analysis
- **Convergence Visualization:** Loss curves and validation metrics
- **A/B Testing Simulation:** Compare different model configurations


---

## üõ†Ô∏è Configuration

The system uses `config.yaml` for centralized configuration:

```yaml
# Data Configuration
data:
  raw_dir: "data/raw/ml-32m"
  processed_dir: "data/processed"
  ratings_file: "ratings.csv"
  movies_file: "movies.csv"
  train_test_split: 0.8
  random_seed: 42

# Model Hyperparameters
models:
  bias_als:
    lambda: 2.0
    n_epochs: 10
    
  full_als:
    n_factors: 15
    lambda: 13.0
    n_epochs: 15
    
  genre_als:
    n_factors: 15
    lambda: 10.0
    tau: 5.0  # Genre constraint weight
    n_epochs: 15
    
  bpr:
    n_factors: 20
    learning_rate: 0.01
    lambda: 0.01
    n_epochs: 30
    n_samples: 1000000

# Training Configuration
training:
  use_numba: true
  parallel: true
  verbose: true
  checkpoint_interval: 5

# Visualization
visualization:
  output_dir: "figures"
  format: "pdf"
  dpi: 300
  style: "whitegrid"
  
# Evaluation
evaluation:
  k_values: [5, 10, 20, 50]
  metrics: ["rmse", "mae", "precision", "recall", "ndcg"]
```

---

## üìä Evaluation Metrics Explained

### Prediction Accuracy Metrics

**RMSE (Root Mean Squared Error):**
$\text{RMSE} = \sqrt{\frac{1}{|\mathcal{T}|} \sum_{(u,i) \in \mathcal{T}} (r_{ui} - \hat{r}_{ui})^2}$

- Measures average prediction error
- Penalizes large errors more heavily
- **Lower is better** (range: 0 to ‚àû)

**MAE (Mean Absolute Error):**
$\text{MAE} = \frac{1}{|\mathcal{T}|} \sum_{(u,i) \in \mathcal{T}} |r_{ui} - \hat{r}_{ui}|$

- More robust to outliers than RMSE
- Interpretable in rating units

### Ranking Quality Metrics

**Precision@K:**
$\text{Precision@K} = \frac{|\text{relevant items in top-K}|}{K}$

- Fraction of recommended items that are relevant
- **Higher is better** (range: 0 to 1)

**Recall@K:**
$\text{Recall@K} = \frac{|\text{relevant items in top-K}|}{|\text{all relevant items}|}$

- Fraction of relevant items that were recommended
- Trade-off with precision

**NDCG@K (Normalized Discounted Cumulative Gain):**
$\text{NDCG@K} = \frac{\text{DCG@K}}{\text{IDCG@K}}$

Where DCG@K = $\sum_{i=1}^{K} \frac{2^{rel_i} - 1}{\log_2(i+1)}$

- Considers ranking position and relevance levels
- **Higher is better** (range: 0 to 1)
- Gold standard for ranking evaluation

### System-Level Metrics

**Coverage:**
- Percentage of catalog items recommended at least once
- Measures diversity of recommendations
- Trade-off with accuracy (popular items easier to recommend)

**Diversity:**
- Average dissimilarity within recommendation lists
- Prevents filter bubbles
- Measured using item-item similarity

**Serendipity:**
- Unexpected but relevant recommendations
- Balances between surprising and accurate

## üìù Academic Report

The full academic report (`Melvin_ALS_report.pdf`) includes:

1. **Introduction** - Problem statement, dataset overview
2. **EDA** - Statistical analysis, power law detection
3. **Methodology** - ALS derivation, optimization techniques
4. **Experiments** - Hyperparameter tuning, ablation studies
5. **Results** - Performance comparison, convergence analysis
6. **Discussion** - Limitations, future work
7. **Conclusion** - Key takeaways


---

## ü§ù Contributing

Contributions are welcome! Please follow these guidelines:

### How to Contribute

1. **Fork the repository**
   ```bash
   git clone https://github.com/MELAI-1/movie-recommender-system.git
   cd movie-recommender-system
   ```

2. **Create a feature branch**
   ```bash
   git checkout -b feature/amazing-feature
   ```

3. **Make your changes**
   - Follow PEP 8 style guidelines
   - Add docstrings to all functions
   - Update documentation as needed
   - Add unit tests for new features

4. **Test your changes**
   ```bash
   python -m pytest tests/
   ```

5. **Commit your changes**
   ```bash
   git commit -m "Add amazing feature: description"
   ```

6. **Push to your fork**
   ```bash
   git push origin feature/amazing-feature
   ```

7. **Open a Pull Request**
   - Provide a clear description of changes
   - Reference any related issues
   - Ensure all tests pass

### Areas for Contribution

- **New Models:** Implement Neural Collaborative Filtering, VAE-CF
- **Optimization:** Further Numba optimizations, GPU support with CuPy
- **Features:** User interface enhancements, API endpoints
- **Documentation:** Tutorials, examples, translations
- **Testing:** Improve test coverage, add benchmarks

---

## üìÑ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

**Summary:**
- ‚úÖ Commercial use allowed
- ‚úÖ Modification allowed
- ‚úÖ Distribution allowed
- ‚úÖ Private use allowed
- ‚ö†Ô∏è Liability and warranty limitations apply

---

## üìß Contact & Support

### Author Information

**Astride Melvin Fokam Ninyim**  
Machine Learning Engineer & Researcher

- üìß **Email:** [melvin@aims.ac.za](mailto:melvin@aims.ac.za)
- üíº **LinkedIn:** [Astride Melvin Fokam Ninyim](https://www.linkedin.com/in/astridemelvinfokamninyim11/)
- üêô **GitHub:** [@MELAI-1](https://github.com/MELAI-1)
- üéì **Affiliation:** AIMS South Africa (African Institute for Mathematical Sciences)

### Getting Help

- üìñ **Documentation Issues:** Open an issue with the `documentation` label
- üêõ **Bug Reports:** Use the bug report template in Issues
- üí° **Feature Requests:** Submit via GitHub Discussions
- ‚ùì **Questions:** Ask in GitHub Discussions or via email

### Project Links

- üåê **Repository:** [https://github.com/MELAI-1/movie-recommender-system](https://github.com/MELAI-1/movie-recommender-system)
- üìä **Project Board:** Track development progress
- üìù **Changelog:** See [CHANGELOG.md](CHANGELOG.md) for version history

---

## üôè Acknowledgments

This project was completed as part of the **Machine Learning at Scale** course at AIMS South Africa.

### Special Thanks

- **Prof. Ulrich Paquet** (DeepMind / AIMS South Africa) - Course instructor and research supervisor
- **GroupLens Research** - For creating and maintaining the MovieLens datasets
- **AIMS South Africa** - For providing computational resources and academic support
- **Numba Development Team** - For the excellent JIT compilation framework
- **Open Source Community** - For NumPy, SciPy, and other foundational libraries

### Academic References

This work builds upon foundational research in collaborative filtering:

1. **Koren, Y., Bell, R., & Volinsky, C.** (2009). *Matrix Factorization Techniques for Recommender Systems.* Computer, 42(8), 30-37.

2. **Rendle, S., Freudenthaler, C., Gantner, Z., & Schmidt-Thieme, L.** (2009). *BPR: Bayesian Personalized Ranking from Implicit Feedback.* UAI.

3. **He, X., Liao, L., Zhang, H., Nie, L., Hu, X., & Chua, T. S.** (2017). *Neural Collaborative Filtering.* WWW.

4. **Harper, F. M., & Konstan, J. A.** (2015). *The MovieLens Datasets: History and Context.* ACM Transactions on Interactive Intelligent Systems (TiiS), 5(4), 19.

5. **Hu, Y., Koren, Y., & Volinsky, C.** (2008). *Collaborative Filtering for Implicit Feedback Datasets.* ICDM.

### Datasets

```bibtex
@article{harper2015movielens,
  title={The MovieLens Datasets: History and Context},
  author={Harper, F. Maxwell and Konstan, Joseph A.},
  journal={ACM Transactions on Interactive Intelligent Systems (TiiS)},
  volume={5},
  number={4},
  pages={1--19},
  year={2015},
  publisher={ACM}
}
```

---

## üéì Academic Report

The complete academic report (`reports/Melvin_ALS_report.pdf`) includes:

### Contents

1. **Introduction**
   - Problem formulation
   - Dataset overview
   - Research objectives

2. **Literature Review**
   - Collaborative filtering evolution
   - Matrix factorization techniques
   - Cold-start problem solutions

3. **Methodology**
   - Mathematical formulation of ALS
   - Optimization techniques
   - Scalability strategies

4. **Exploratory Data Analysis**
   - Power law analysis
   - Temporal patterns
   - Genre topology

5. **Experiments**
   - Hyperparameter tuning
   - Model comparison
   - Ablation studies

6. **Results & Discussion**
   - Performance benchmarks
   - Convergence analysis
   - Cold-start evaluation

7. **Conclusion**
   - Key findings
   - Limitations
   - Future work

8. **Appendix**
   - Code snippets
   - Additional visualizations
   - Mathematical derivations

---

## üìö Additional Resources

### Tutorials

- [Understanding Matrix Factorization](docs/tutorials/matrix_factorization.md)
- [Numba Optimization Guide](docs/tutorials/numba_guide.md)
- [Streamlit Dashboard Development](docs/tutorials/streamlit_guide.md)

### Example Notebooks

- `notebooks/01_eda.ipynb` - Complete exploratory data analysis
- `notebooks/02_bias_als.ipynb` - Bias-only baseline implementation
- `notebooks/03_full_als.ipynb` - Full matrix factorization with Numba

### API Documentation

```python
# Example usage
from src.models.als_full import MatrixFactorizationALS
from src.data_loader import DataLoader

# Load data
loader = DataLoader("data/raw/ml-32m")
R_train, R_test = loader.get_train_test_split()

# Train model
model = MatrixFactorizationALS(n_factors=15, lambda_reg=13.0)
model.fit(R_train, R_test)

# Get recommendations
user_id = 42
top_items, scores = model.get_top_items(user_id, n_items=10)
```

---

**Built with ‚ù§Ô∏è for Machine Learning at Scale**  
*AIMS South Africa - Class of 2025*

---

**Last Updated:** January 2026  
**Version:** 1.0.0  
**Status:** Production Ready ‚úÖ
